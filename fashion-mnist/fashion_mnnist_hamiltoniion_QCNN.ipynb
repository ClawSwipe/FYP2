{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52CR7KmybQYr",
        "outputId": "7fb537bd-d4d9-4c7e-c01b-85fdcad33c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.11/dist-packages (0.40.0)\n",
            "Requirement already satisfied: pennylane-lightning in /usr/local/lib/python3.11/dist-packages (0.40.0)\n",
            "Requirement already satisfied: cotengra in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: quimb in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.16.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.7.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning) (0.3.29.0.0)\n",
            "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning[gpu]) (0.40.0)\n",
            "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from quimb) (1.0.1)\n",
            "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.11/dist-packages (from quimb) (0.61.0)\n",
            "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from quimb) (5.9.5)\n",
            "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.11/dist-packages (from quimb) (4.67.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.39->quimb) (0.44.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.5.0)\n",
            "Requirement already satisfied: custatevec-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (1.7.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.1.3)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda12_pip] in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: jaxlib<=0.5.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax[cuda12_pip]) (0.5.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax[cuda12_pip]) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax[cuda12_pip]) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax[cuda12_pip]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax[cuda12_pip]) (1.13.1)\n",
            "Requirement already satisfied: jax-cuda12-plugin<=0.5.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (0.5.0)\n",
            "Requirement already satisfied: jax-cuda12-pjrt==0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin<=0.5.0,>=0.5.0->jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (0.5.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.6.85 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (9.3.0.75)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (11.2.3.61)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.5.1.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.5.82)\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb --upgrade\n",
        "!pip install -U jax[cuda12]==0.5.3 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "# !pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC92azd1bgkU",
        "outputId": "3f7d377a-89f5-4137-aaab-13354fc5f097"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, `baytune (btb)`, `chocolate`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, `baytune (btb)`, `chocolate`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.\n",
            "  warnings.warn(\n",
            "<ipython-input-3-b00b07bf67a6>:40: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  tpu_backend = xla_bridge.get_backend('tpu')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set platform to GPU\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "#np.set_printoptions(threshold=sys.maxsize)\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "import jax\n",
        "import time\n",
        "\n",
        "import functools\n",
        "\n",
        "from typing import List, Union, Tuple, Dict, Optional, Any\n",
        "from typing import Callable\n",
        "\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "#jax.config.update(\"jax_debug_nans\", True)\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import optax  # optimization using jax\n",
        "\n",
        "import torch  # https://pytorch.org\n",
        "import torchvision  # https://pytorch.org\n",
        "#torch.set_printoptions(profile=\"full\")\n",
        "import pennylane as qml\n",
        "import pennylane.numpy as pnp\n",
        "\n",
        "import os, cv2, itertools # cv2 -- OpenCV\n",
        "import shutil\n",
        "import zipfile\n",
        "%matplotlib inline\n",
        "\n",
        "from jax.lib import xla_bridge\n",
        "\n",
        "def set_jax_platform():\n",
        "    # Check if TPU is available\n",
        "    try:\n",
        "        tpu_backend = xla_bridge.get_backend('tpu')\n",
        "        if tpu_backend and tpu_backend.device_count() > 0:\n",
        "            # Set platform to TPU\n",
        "            jax.config.update('jax_platform_name', 'tpu')\n",
        "            print(\"Set platform to TPU\")\n",
        "            return\n",
        "    except RuntimeError:\n",
        "        pass  # No TPU found, move on to check for GPU\n",
        "\n",
        "    # Check if GPU is available\n",
        "    try:\n",
        "      gpu_backend = xla_bridge.get_backend('gpu')\n",
        "      if gpu_backend and gpu_backend.device_count() > 0:\n",
        "          # Set platform to CUDA (GPU)\n",
        "          jax.config.update('jax_platform_name', 'gpu')\n",
        "          print(\"Set platform to GPU\")\n",
        "    except RuntimeError:\n",
        "          # Set platform to CPU\n",
        "          jax.config.update('jax_platform_name', 'cpu')\n",
        "          print(\"Set platform to CPU\")\n",
        "\n",
        "# Call the function to set the platform\n",
        "set_jax_platform()\n",
        "\n",
        "sns.set()\n",
        "\n",
        "seed = 1701\n",
        "rng = np.random.default_rng(seed=seed)\n",
        "prng = pnp.random.default_rng(seed=seed)\n",
        "jrng_key = jax.random.PRNGKey(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCo8fdobbiTB"
      },
      "source": [
        "# Prepare the Dataset\n",
        "\n",
        "For the rescaled image matrix $M$, the \"Hermitian version\" of it can be calculated as:\n",
        "\n",
        "$$\n",
        "A = \\frac{M+M^T}{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlSUCStebkoA",
        "outputId": "3e573026-5ef1-40c5-ca88-536c6abc6b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 32, 32)\n",
            "(64,)\n",
            "[1 4 7 3 8 5 9 8 3 6 8 3 5 4 0 1 6 6 3 9 8 4 9 3 9 0 0 5 4 5 9 6 3 7 9 5 4\n",
            " 6 2 8 9 3 4 7 9 7 2 8 9 7 5 9 5 5 5 2 2 4 1 0 6 1 4 0]\n",
            "[0.         0.         0.27058825 0.4627451  0.44509804 0.4490196\n",
            " 0.44705883 0.44705883 0.45294118 0.68039215 1.         0.8411765\n",
            " 0.4490196  0.4254902  0.5        0.30980393 0.         0.42745098\n",
            " 0.4862745  0.44509804 0.45294118 0.43333334 0.49215686 0.25686276\n",
            " 0.         0.         0.         0.         0.00392157 0.\n",
            " 0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Pad(2),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
        "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
        "    torchvision.transforms.Lambda(lambda x: (x+torch.transpose(x, 0, 1))/2)\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    \"FashioMNIST\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=preprocess,\n",
        ")\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    \"FashioMNIST\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=preprocess,\n",
        ")\n",
        "dummy_trainloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "dummy_testloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
        "dummy_x = dummy_x.numpy()\n",
        "dummy_y = dummy_y.numpy()\n",
        "print(dummy_x.shape)  # 64x32x32\n",
        "print(dummy_y.shape)  # 64\n",
        "print(dummy_y)\n",
        "print(dummy_x[0,16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR5l9DZ5bubn"
      },
      "source": [
        "# Time-Evolve the Image Hermitian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbF2gRNbxKF",
        "outputId": "212cd98f-c48b-415b-f622-2e63637ad16b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.        +0.j          0.        +0.j         -0.02429903-0.01684894j\n",
            " -0.03731519-0.03757089j -0.22626635-0.06169768j -0.1700719 -0.06010128j\n",
            " -0.16882206-0.04524178j -0.14803158-0.05492992j -0.12771498-0.06791211j\n",
            " -0.1698251 -0.05806413j -0.13397437-0.10630392j -0.06927654-0.520455j\n",
            " -0.0243065 +0.11732264j -0.01479168+0.18684916j  0.13696264+0.06661594j\n",
            " -0.11863886+0.36411586j  0.02026659+0.2327596j   0.06111977+0.18984471j\n",
            "  0.1877227 -0.01277907j -0.1176137 -0.02013119j -0.05401535+0.01873714j\n",
            "  0.0539494 -0.14361155j -0.13630159+0.15736258j  0.02253049+0.10692517j\n",
            "  0.11916239+0.01191842j  0.1479688 -0.01442357j  0.08228312-0.0131961j\n",
            "  0.00868747-0.00502298j  0.05494964-0.03305431j -0.04707357+0.00085689j\n",
            "  0.        +0.j          0.        +0.j        ]\n",
            "[[ 1.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j\n",
            "   0.0000000e+00+0.0000000e+00j ...  0.0000000e+00+0.0000000e+00j\n",
            "   0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j]\n",
            " [ 0.0000000e+00+0.0000000e+00j  1.0000000e+00+0.0000000e+00j\n",
            "   0.0000000e+00+0.0000000e+00j ...  0.0000000e+00+0.0000000e+00j\n",
            "   0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j]\n",
            " [ 0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j\n",
            "   1.0000001e+00+1.8085102e-09j ... -2.9284749e-07+3.0646137e-07j\n",
            "   0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j]\n",
            " ...\n",
            " [ 0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j\n",
            "  -2.7950944e-07-3.0646137e-07j ...  1.0000042e+00+1.9366886e-10j\n",
            "   0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j]\n",
            " [ 0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j\n",
            "   0.0000000e+00+0.0000000e+00j ...  0.0000000e+00+0.0000000e+00j\n",
            "   1.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j]\n",
            " [ 0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j\n",
            "   0.0000000e+00+0.0000000e+00j ...  0.0000000e+00+0.0000000e+00j\n",
            "   0.0000000e+00+0.0000000e+00j  1.0000000e+00+0.0000000e+00j]]\n",
            "[[ 1.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j\n",
            "   0.0000000e+00+0.000000e+00j ...  0.0000000e+00+0.000000e+00j\n",
            "   0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j]\n",
            " [ 0.0000000e+00+0.000000e+00j  1.0000000e+00+0.000000e+00j\n",
            "   0.0000000e+00+0.000000e+00j ...  0.0000000e+00+0.000000e+00j\n",
            "   0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j]\n",
            " [ 0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j\n",
            "   1.0000002e+00+4.775472e-10j ... -3.1221779e-07-8.154533e-09j\n",
            "   0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j]\n",
            " ...\n",
            " [ 0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j\n",
            "  -3.1100458e-07+8.154533e-09j ...  1.0000042e+00-2.056314e-09j\n",
            "   0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j]\n",
            " [ 0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j\n",
            "   0.0000000e+00+0.000000e+00j ...  0.0000000e+00+0.000000e+00j\n",
            "   1.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j]\n",
            " [ 0.0000000e+00+0.000000e+00j  0.0000000e+00+0.000000e+00j\n",
            "   0.0000000e+00+0.000000e+00j ...  0.0000000e+00+0.000000e+00j\n",
            "   0.0000000e+00+0.000000e+00j  1.0000000e+00+0.000000e+00j]]\n"
          ]
        }
      ],
      "source": [
        "def img_hermitian_evolve(\n",
        "    img:jnp.ndarray,\n",
        "    t:float\n",
        ")->jnp.ndarray:\n",
        "  assert img.shape[-1]==32 and img.shape[-2] == 32, f\"The shape of the image must be 32 by 32, got {img.shape[-2]} by {img.shape[-1]}\"\n",
        "  return jax.scipy.linalg.expm(img*( -0.5j*t))\n",
        "\n",
        "print(\n",
        "    img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )[16]\n",
        "    )\n",
        "\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        jnp.transpose(jnp.conjugate(img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        ))),\n",
        "        img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        ),\n",
        "        jnp.transpose(jnp.conjugate(img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )))\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enL2qIVtcNLF"
      },
      "source": [
        "# Some Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xVOKRfoacK60"
      },
      "outputs": [],
      "source": [
        "ket = {\n",
        "    '0':jnp.array([1,0]),\n",
        "    '1':jnp.array([0,1]),\n",
        "    '+':(jnp.array([1,0]) + jnp.array([0,1]))/jnp.sqrt(2),\n",
        "    '-':(jnp.array([1,0]) - jnp.array([0,1]))/jnp.sqrt(2)\n",
        "}\n",
        "\n",
        "pauli = {\n",
        "    'I':jnp.array([[1,0],[0,1]]),\n",
        "    'X':jnp.array([[0,1],[1,0]]),\n",
        "    'Y':jnp.array([[0, -1j],[1j, 0]]),\n",
        "    'Z':jnp.array([[1,0],[0,-1]])\n",
        "}\n",
        "\n",
        "def tensor_product(*args):\n",
        "  input_list = [a for a in args]\n",
        "  return functools.reduce(jnp.kron, input_list)\n",
        "\n",
        "def multi_qubit_identity(n_qubits:int)->jnp.ndarray:\n",
        "  assert n_qubits>0\n",
        "  if n_qubits == 1:\n",
        "    return pauli['I']\n",
        "  else:\n",
        "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
        "\n",
        "pauli_words_su4 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
        "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
        "\n",
        "pauli_words_su8 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      if not key1+key2+key3 == 'III':\n",
        "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
        "\n",
        "pauli_words_su16 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      for key4 in pauli.keys():\n",
        "        if not key1+key2+key3+key4 == 'IIII':\n",
        "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
        "              pauli[key1],\n",
        "              pauli[key2],\n",
        "              pauli[key3],\n",
        "              pauli[key4]\n",
        "          )\n",
        "\n",
        "pauli_words_su32 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      for key4 in pauli.keys():\n",
        "        for key5 in pauli.keys():\n",
        "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
        "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
        "                pauli[key1],\n",
        "                pauli[key2],\n",
        "                pauli[key3],\n",
        "                pauli[key4],\n",
        "                pauli[key5]\n",
        "            )\n",
        "\n",
        "observables_10_cls_5q = [0]*10\n",
        "for i in ['0', '1']:\n",
        "  for j in ['0', '1']:\n",
        "    for k in ['0', '1']:\n",
        "      for l in ['0', '1']:\n",
        "        idx = int(i+j+k+l, 2)\n",
        "        if idx <10:\n",
        "          basis_state = tensor_product(*[ket[i], ket[j], ket[k], ket[l]])\n",
        "          four_qubit_obs = jnp.outer(basis_state, basis_state)\n",
        "          observables_10_cls_5q[idx] = tensor_product(four_qubit_obs, multi_qubit_identity(1))\n",
        "\n",
        "observables_8_cls_5q = [0]*8\n",
        "for i in ['0', '1']:\n",
        "  for j in ['0', '1']:\n",
        "    for k in ['0', '1']:\n",
        "      for l in ['0', '1']:\n",
        "        idx = int(i+j+k+l, 2)\n",
        "        if idx <8:\n",
        "          basis_state = tensor_product(*[ket[i], ket[j], ket[k], ket[l]])\n",
        "          four_qubit_obs = jnp.outer(basis_state, basis_state)\n",
        "          observables_8_cls_5q[idx] = tensor_product(four_qubit_obs, multi_qubit_identity(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfMfQrSncpW1",
        "outputId": "bb491c42-4dc2-41fa-8f79-d372201fb083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.00000000e+00-2.49661392e-18j -5.03178228e-16-2.04870843e-15j\n",
            "  -8.74300632e-16-5.48389459e-16j ...  7.80625564e-17-3.81639165e-17j\n",
            "   6.10622664e-16-1.80411242e-16j  1.68962067e-15-2.22044605e-16j]\n",
            " [-5.03178228e-16+2.04523898e-15j  1.00000000e+00-1.05095838e-18j\n",
            "   5.55111512e-16-5.75928194e-16j ...  4.09394740e-16-2.77555756e-16j\n",
            "   4.78783679e-16-8.46545056e-16j -9.71445147e-17+3.88578059e-16j]\n",
            " [-8.74300632e-16+5.37330597e-16j  5.55111512e-16+5.75928194e-16j\n",
            "   1.00000000e+00+6.27145069e-18j ...  1.42247325e-16-7.35522754e-16j\n",
            "   1.17961196e-15+4.85722573e-16j  7.04297731e-16+3.46944695e-17j]\n",
            " ...\n",
            " [ 5.37764278e-17+3.81639165e-17j  4.02455846e-16+2.77555756e-16j\n",
            "   1.40512602e-16+7.35522754e-16j ...  1.00000000e+00-8.04795179e-18j\n",
            "  -3.46944695e-17+1.80411242e-16j -3.60822483e-16+9.29811783e-16j]\n",
            " [ 6.17561557e-16+1.80411242e-16j  4.57966998e-16+8.46545056e-16j\n",
            "   1.17961196e-15-4.85722573e-16j ... -3.46944695e-17-1.80411242e-16j\n",
            "   1.00000000e+00-1.34708622e-18j  8.84708973e-17+6.33174069e-16j]\n",
            " [ 1.69655956e-15+2.22044605e-16j -1.11022302e-16-3.88578059e-16j\n",
            "   7.18175519e-16-3.46944695e-17j ... -3.60822483e-16-9.02056208e-16j\n",
            "   8.84708973e-17-6.40112963e-16j  1.00000000e+00-8.60541226e-18j]]\n",
            "[[ 1.00000000e+00-1.45304194e-17j  6.24500451e-16-4.51895465e-16j\n",
            "  -6.48786580e-16-2.28983499e-16j ...  9.28077060e-16+4.23272528e-16j\n",
            "  -4.44089210e-16+7.07767178e-16j  1.84574578e-15-3.36536354e-16j]\n",
            " [ 6.24500451e-16+4.44089210e-16j  1.00000000e+00+4.13988226e-19j\n",
            "   7.91033905e-16+6.10622664e-16j ...  1.88217497e-16-8.32667268e-16j\n",
            "  -1.52655666e-16+2.63677968e-16j -4.85722573e-17-9.71445147e-17j]\n",
            " [-6.48786580e-16+2.25514052e-16j  7.91033905e-16-5.96744876e-16j\n",
            "   1.00000000e+00-8.29605981e-18j ...  2.08166817e-16-7.21644966e-16j\n",
            "   1.04083409e-15+2.01227923e-16j -3.46944695e-16+1.06858966e-15j]\n",
            " ...\n",
            " [ 9.03790931e-16-4.23272528e-16j  1.90819582e-16+8.32667268e-16j\n",
            "   1.94289029e-16+7.21644966e-16j ...  1.00000000e+00-2.05061419e-18j\n",
            "  -9.15933995e-16+1.32532874e-15j -5.13478149e-16-4.51028104e-16j]\n",
            " [-4.71844785e-16-7.07767178e-16j -1.59594560e-16-2.63677968e-16j\n",
            "   1.04083409e-15-2.01227923e-16j ... -9.15933995e-16-1.34614542e-15j\n",
            "   1.00000000e+00-1.75600516e-18j  4.92661467e-16+4.71844785e-16j]\n",
            " [ 1.85268467e-15+3.36536354e-16j -7.63278329e-17+9.71445147e-17j\n",
            "  -3.60822483e-16-1.06858966e-15j ... -5.13478149e-16+4.44089210e-16j\n",
            "   4.92661467e-16-4.71844785e-16j  1.00000000e+00+4.49169186e-18j]]\n"
          ]
        }
      ],
      "source": [
        "def su32_op(\n",
        "    params:jnp.ndarray\n",
        "):\n",
        "  generator = jnp.einsum(\"i, ijk - >jk\", params, jnp.asarray(list(pauli_words_su32.values())))\n",
        "  return jax.scipy.linalg.expm(1j*generator)\n",
        "\n",
        "test_params = jax.random.normal(shape=[4**5-1], key=jrng_key)\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        jnp.transpose(jnp.conjugate(su32_op(test_params))),\n",
        "        su32_op(test_params)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        su32_op(test_params),\n",
        "        jnp.transpose(jnp.conjugate(su32_op(test_params)))\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cJ8epBAvc_77"
      },
      "outputs": [],
      "source": [
        "def measure_sv(\n",
        "    state:jnp.ndarray,\n",
        "    observable:jnp.ndarray\n",
        "    ):\n",
        "  \"\"\"\n",
        "  Measure a statevector with a Hermitian observable.\n",
        "  Note: No checking Hermitianicity of the observable or whether the observable\n",
        "  has all real eigenvalues or not\n",
        "  \"\"\"\n",
        "  expectation_value = jnp.dot(jnp.conj(state.T), jnp.dot(observable, state))\n",
        "  return jnp.real(expectation_value)\n",
        "\n",
        "def measure_dm(\n",
        "    rho:jnp.ndarray,\n",
        "    observable:jnp.ndarray\n",
        "):\n",
        "  \"\"\"\n",
        "  Measure a density matrix with a Hermitian observable.\n",
        "  Note: No checking Hermitianicity of the observable or whether the observable\n",
        "  has all real eigenvalues or not.\n",
        "  \"\"\"\n",
        "  product = jnp.dot(rho, observable)\n",
        "\n",
        "  # Calculate the trace, which is the sum of diagonal elements\n",
        "  trace = jnp.trace(product)\n",
        "\n",
        "  # The expectation value should be real for physical observables\n",
        "  return jnp.real(trace)\n",
        "\n",
        "vmap_measure_sv = jax.vmap(measure_sv, in_axes=(None, 0), out_axes=0)\n",
        "vmap_measure_dm = jax.vmap(measure_dm, in_axes=(None, 0), out_axes=0)\n",
        "\n",
        "def bitstring_to_state(bitstring:str):\n",
        "  \"\"\"\n",
        "  Convert a bit string, like '0101001' or '+-+-101'\n",
        "  to a statevector. Each character in the bitstring must be among\n",
        "  0, 1, + and -\n",
        "  \"\"\"\n",
        "  assert len(bitstring)>0\n",
        "  for c in bitstring:\n",
        "    assert c in ['0', '1', '+', '-']\n",
        "  single_qubit_states = [ket[c] for c in bitstring]\n",
        "  return tensor_product(*single_qubit_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xn852rlH916"
      },
      "source": [
        "#QCNN with Interaction Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFjfEttbdNEn"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UShV72-PXk-B",
        "outputId": "22ee7f2e-22ef-4a1f-951f-27c503bef899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "15\n",
            "(8,)\n",
            "0\n",
            "2\n",
            "(8,)\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import vmap, lax\n",
        "from jax import random\n",
        "from jax.image import resize\n",
        "\n",
        "# Assuming these are defined elsewhere in your code:\n",
        "# ket = {'+': ...} # Define your ket states\n",
        "# observables_8_cls_5q = ... # Define your observables for measurement\n",
        "# su32_op = ... # Define your SU(32) operator function\n",
        "# img_hermitian_evolve = ... # Define your Hamiltonian evolution function\n",
        "# tensor_product = ... # Define your tensor product function\n",
        "# vmap_measure_sv = ... # Define your measurement function\n",
        "\n",
        "def local_img_hermitian_evolve(img_patch: jnp.ndarray, t: float):\n",
        "    return img_hermitian_evolve(img_patch, t)\n",
        "\n",
        "def conv_layer(img: jnp.ndarray, params: jnp.ndarray, t: jnp.ndarray,\n",
        "               patch_size: int, stride: int):\n",
        "    img_height, img_width = img.shape\n",
        "    num_patches_height = (img_height - patch_size) // stride + 1\n",
        "    num_patches_width = (img_width - patch_size) // stride + 1\n",
        "\n",
        "    single_op_params = 4**5 - 1\n",
        "    n_inner_layers = len(params) // single_op_params // (num_patches_height * num_patches_width)\n",
        "\n",
        "    def process_patch(row, col):\n",
        "        start_indices = (row * stride, col * stride)\n",
        "        slice_sizes = (patch_size, patch_size)\n",
        "\n",
        "        # Extract and resize patch dynamically\n",
        "        img_patch = lax.dynamic_slice(img, start_indices, slice_sizes)\n",
        "        img_patch_resized = resize(img_patch, (32, 32), method=\"bilinear\")\n",
        "\n",
        "        state = tensor_product(ket['+'], ket['+'], ket['+'], ket['+'], ket['+'])\n",
        "\n",
        "        state = jnp.dot(local_img_hermitian_evolve(img_patch_resized, t), state)\n",
        "\n",
        "        inner_layer_params = params[:n_inner_layers * single_op_params]\n",
        "\n",
        "        for j in range(n_inner_layers):\n",
        "            state = jnp.dot(\n",
        "                su32_op(inner_layer_params[j * single_op_params:(j + 1) * single_op_params]),\n",
        "                state,\n",
        "            )\n",
        "\n",
        "        return state\n",
        "\n",
        "    patch_coords = [(row, col) for row in range(num_patches_height)\n",
        "                    for col in range(num_patches_width)]\n",
        "\n",
        "    processed_patches = vmap(lambda coord: process_patch(*coord))(jnp.array(patch_coords))\n",
        "\n",
        "    return processed_patches\n",
        "def qnn_hamevo(\n",
        "    params:jnp.ndarray,\n",
        "    t:jnp.ndarray,\n",
        "    img:jnp.ndarray\n",
        ")->jnp.ndarray:\n",
        "  \"\"\"\n",
        "  A QNN that takes (M+M^T)/2\n",
        "  as input, where M is the (rescaled) original image,\n",
        "  as well as a trainable parameter t,\n",
        "  and parameters for trainable layers\n",
        "  and output an array of 2 elements representing classification logits\n",
        "  \"\"\"\n",
        "  single_op_params = 4**5-1\n",
        "\n",
        "  n_outer_layers = len(t)\n",
        "  n_inner_layers = (len(params)//single_op_params)//n_outer_layers\n",
        "  print(n_inner_layers)\n",
        "  print(n_outer_layers)\n",
        "  state = tensor_product(ket['+'], ket['+'], ket['+'], ket['+'], ket['+'])\n",
        "  for i in range(n_outer_layers):\n",
        "    state = jnp.dot(\n",
        "      img_hermitian_evolve(img, t[i]),\n",
        "      state\n",
        "      )\n",
        "    inner_layer_params = params[i*(single_op_params*n_inner_layers):(i+1)*(single_op_params*n_inner_layers)]\n",
        "    for j in range(n_inner_layers):\n",
        "      state = jnp.dot(\n",
        "          #brickwall_su4_5q_single_layer(inner_layer_params[j*single_op_params:(j+1)*single_op_params]),\n",
        "          su32_op(inner_layer_params[j*single_op_params:(j+1)*single_op_params]),\n",
        "          state\n",
        "      )\n",
        "  return vmap_measure_sv(state, jnp.asarray(observables_8_cls_5q))\n",
        "\n",
        "\n",
        "\n",
        "print(\n",
        "    qnn_hamevo(\n",
        "        jax.random.normal(shape=[( 4**5-1)*15], key=jrng_key),\n",
        "        jax.random.normal(shape=[15], key=jrng_key),\n",
        "        dummy_x[0]\n",
        "    ).shape\n",
        ")\n",
        "\n",
        "def quantum_pooling(states: jnp.ndarray, pool_size: int):\n",
        "    \"\"\"\n",
        "    Performs quantum pooling to reduce the number of qubits.\n",
        "\n",
        "    Args:\n",
        "        states: The quantum states to pool.\n",
        "        pool_size: The size of the pooling region.\n",
        "\n",
        "    Returns:\n",
        "        A list of pooled quantum states.\n",
        "    \"\"\"\n",
        "    num_patches = states.shape[0]\n",
        "    num_pools = num_patches // pool_size\n",
        "\n",
        "    # Correctly reshape the input to combine states in each pool\n",
        "    reshaped_states = states[:num_pools * pool_size].reshape(num_pools, pool_size, *states.shape[1:])\n",
        "\n",
        "    # Perform the pooling operation (e.g., averaging) over the pools\n",
        "    pooled_states = jnp.mean(reshaped_states, axis=1)\n",
        "\n",
        "    return pooled_states\n",
        "\n",
        "\n",
        "def qcnn_hamevo(params: jnp.ndarray, t_conv: jnp.ndarray, t_final: float,\n",
        "                img: jnp.ndarray, patch_size: int, stride: int,\n",
        "                pool_size: int, n_conv_layers: int,\n",
        "                n_outer_layers: int):\n",
        "\n",
        "    single_op_params = 4**5 - 1\n",
        "\n",
        "    # Convolutional Layers\n",
        "    all_patches = img\n",
        "    conv_layer_params_start = 0\n",
        "\n",
        "    for i in range(n_conv_layers):\n",
        "        n_inner_layers = len(params) // single_op_params // n_conv_layers\n",
        "\n",
        "        conv_layer_params_end = conv_layer_params_start + n_inner_layers * single_op_params\n",
        "\n",
        "        conv_layer_params = params[conv_layer_params_start : conv_layer_params_end]\n",
        "\n",
        "        all_patches = conv_layer(all_patches, conv_layer_params, t_conv[i], patch_size, stride)\n",
        "\n",
        "        conv_layer_params_start += n_inner_layers * single_op_params\n",
        "\n",
        "    # Pooling Layers\n",
        "    pooled_patches = quantum_pooling(all_patches, pool_size)\n",
        "\n",
        "    # Outer Trainable layers\n",
        "    n_inner_layers_outer = (len(params) - conv_layer_params_start) // single_op_params // n_outer_layers\n",
        "\n",
        "    # Aggregate pooled patches into a single vector of shape (32,)\n",
        "    final_state = jnp.mean(pooled_patches, axis=0)\n",
        "\n",
        "    # Reshape to (1, 32) before resizing, to make it a 2D array\n",
        "    final_state = jnp.reshape(final_state, (1, 32))\n",
        "\n",
        "    # Resize to (32, 32)\n",
        "    final_state = resize(final_state, (32, 32), method=\"bilinear\")\n",
        "\n",
        "    # Reshape to (32, 32)\n",
        "    final_state = jnp.reshape(final_state, (32, 32))\n",
        "\n",
        "    for i in range(n_outer_layers):\n",
        "        evolved_state = local_img_hermitian_evolve(final_state, t_final)\n",
        "\n",
        "        for j in range(n_inner_layers_outer):\n",
        "            inner_layer_params_start = conv_layer_params_start + i * (single_op_params * n_inner_layers_outer)\n",
        "            inner_layer_params_end = inner_layer_params_start + (single_op_params * n_inner_layers_outer)\n",
        "            inner_layer_params = params[inner_layer_params_start : inner_layer_params_end]\n",
        "\n",
        "            evolved_state = jnp.dot(\n",
        "                su32_op(inner_layer_params[j * single_op_params:(j + 1) * single_op_params]),\n",
        "                evolved_state\n",
        "            )\n",
        "\n",
        "        final_state = evolved_state\n",
        "\n",
        "\n",
        "    return qnn_hamevo(\n",
        "        params,\n",
        "        t_conv,\n",
        "        img\n",
        "    )\n",
        "\n",
        "\n",
        "# Example usage (assuming dummy_x is defined elsewhere)\n",
        "key = random.PRNGKey(0)\n",
        "img_height, img_width = 32, 32  # Example image dimensions\n",
        "dummy_x = random.normal(key, (1, img_height, img_width))\n",
        "patch_size = 7\n",
        "stride = 2\n",
        "pool_size = 4\n",
        "n_conv_layers = 2\n",
        "n_outer_layers = 15\n",
        "\n",
        "#Example static value; adjust based on your model design.\n",
        "num_conv_params = 100\n",
        "#Example static value; adjust based on your model design.\n",
        "num_outer_params = 50\n",
        "\n",
        "# Total number of params for the conv and outer layers combined.\n",
        "total_params = num_conv_params + num_outer_params\n",
        "\n",
        "output = qcnn_hamevo(\n",
        "    params=random.normal(key, shape=[total_params]),  # Total number of parameters.\n",
        "    t_conv=random.normal(key, shape=[n_conv_layers]),  # Time for each conv layer.\n",
        "    t_final=random.normal(key),  # Time for outer trainable layer.\n",
        "    img=dummy_x[0],              # Input image.\n",
        "    patch_size=patch_size,\n",
        "    stride=stride,\n",
        "    pool_size=pool_size,\n",
        "    n_conv_layers=n_conv_layers,\n",
        "    n_outer_layers=n_outer_layers,\n",
        ")\n",
        "\n",
        "print(output.shape )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6Pc1I3fqdOX_"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def compute_out(weight, t, features, labels):\n",
        "\n",
        "    patch_size = 7\n",
        "    stride = 2\n",
        "    pool_size = 4\n",
        "    key = random.PRNGKey(0)\n",
        "    n_conv_layers = 2\n",
        "    n_outer_layers = 8\n",
        "\n",
        "    \"\"\"Computes the output of the corresponding label in the qcnn\"\"\"\n",
        "    out = lambda weight, t, feature, label: qcnn_hamevo(\n",
        "        weight,\n",
        "        t_conv=t,\n",
        "        t_final=jax.random.normal(key),\n",
        "        img=feature,\n",
        "        patch_size=patch_size,\n",
        "        stride=stride,\n",
        "        pool_size=pool_size,\n",
        "        n_conv_layers=n_conv_layers,\n",
        "        n_outer_layers=n_outer_layers,\n",
        "    )\n",
        "\n",
        "    return jax.vmap(out, in_axes=(None, None, 0, 0), out_axes=0)(\n",
        "        weight, t, features, labels\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_accuracy(weight,t, features, labels):\n",
        "    \"\"\"Computes the accuracy over the provided features and labels\"\"\"\n",
        "    out = compute_out(weight,t, features, labels)\n",
        "    pred = jnp.argmax(out, axis = 1)\n",
        "    return jnp.sum(jnp.array(pred == labels).astype(int)) / len(out)\n",
        "\n",
        "\n",
        "def compute_cost(weight,t, features, labels):\n",
        "    \"\"\"Computes the cost over the provided features and labels\"\"\"\n",
        "    logits = compute_out(weight,t, features, labels)\n",
        "    return jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(logits, labels))\n",
        "\n",
        "\n",
        "value_and_grad = jax.jit(jax.value_and_grad(compute_cost, argnums=[0,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aI13rv6DdRJO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "N_OUTER_LAYERS = 10\n",
        "N_INNER_LAYERS = 1\n",
        "N_LAYERS = N_OUTER_LAYERS*N_INNER_LAYERS\n",
        "SINGLE_OP_PARAMS  = 4**5-1\n",
        "\n",
        "def init_weights():\n",
        "    return jax.random.normal(shape=[SINGLE_OP_PARAMS*N_LAYERS], key=jrng_key),jax.random.normal(shape=[N_OUTER_LAYERS], key=jrng_key)\n",
        "\n",
        "# def init_weights(alpha=0.5, beta=2.0):\n",
        "#     df = pd.read_csv('weights_beta.csv')\n",
        "\n",
        "# # Check if the DataFrame is not empty\n",
        "#     if not df.empty:\n",
        "#     # Access the last row directly\n",
        "#        row = df.iloc[-1]  # Access the last row\n",
        "\n",
        "#     # Extract the epoch, weights, and biases\n",
        "#     epoch = row['epoch']\n",
        "#     weights = jnp.array(ast.literal_eval(row['weights']))  # Convert to JAX array\n",
        "#     biases = jnp.array(ast.literal_eval(row['biases']))    # Convert to JAX array\n",
        "#     # Initialize weights with a Beta distribution skewed towards 0\n",
        "#     # weights = jax.random.beta(jrng_key, alpha, beta, shape=[SINGLE_OP_PARAMS*N_LAYERS])\n",
        "#     # biases = jax.random.beta(jrng_key, alpha, beta, shape=[N_OUTER_LAYERS])\n",
        "#     return weights, biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ftq2tMNou7a6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def save_weights_to_csv(weights, biases, epoch, file_name='weights_beta.csv'):\n",
        "    \"\"\"Saves the weights and biases to a CSV file.\"\"\"\n",
        "    # Convert weights and biases to a flat list\n",
        "    weight_list = weights.flatten().tolist()\n",
        "    bias_list = biases.flatten().tolist()\n",
        "\n",
        "    # Create a dictionary to store the weights and biases with epoch\n",
        "    data = {'epoch': [epoch], 'weights': [weight_list], 'biases': [bias_list]}\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Check if the file exists before appending\n",
        "    if not os.path.isfile(file_name):\n",
        "        # If the file does not exist, create it with a header\n",
        "        df.to_csv(file_name, mode='w', header=True, index=False)\n",
        "    else:\n",
        "        # If the file exists, append the new data without a header\n",
        "        df.to_csv(file_name, mode='a', header=False, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU8hJrkM6OFO",
        "outputId": "8bcf1a99-198a-4c4e-9ee7-7e2154ea4f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with: 45600; Validation test: 2400; Testing with: 8000\n"
          ]
        }
      ],
      "source": [
        "# Select data\n",
        "labels = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "indices_train = [idx for idx, target in enumerate(train_dataset.targets) if target in labels]\n",
        "\n",
        "# Separate 5% of the training data for validation\n",
        "val_size = int(0.05 * len(indices_train))\n",
        "indices_val = indices_train[:val_size]  # First 5% for validation\n",
        "indices_train = indices_train[val_size:]  # Remaining 95% for training\n",
        "\n",
        "indices_test = [idx for idx, target in enumerate(test_dataset.targets) if target in labels]\n",
        "\n",
        "N_TRAIN = len(indices_train)\n",
        "N_VAL = len(indices_val)\n",
        "N_TEST = len(indices_test)\n",
        "\n",
        "print(\n",
        "    f\"Training with: {N_TRAIN}; Validation test: {N_VAL}; Testing with: {N_TEST}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLyRg7EydVgN"
      },
      "outputs": [],
      "source": [
        "def train_vqc(batchsize: int, n_epochs: int, seed: int = 1701):\n",
        "    start = time.time()\n",
        "    np.random.seed(seed)\n",
        "    key = random.PRNGKey(0)  # Set a random seed (change 0 for different results)\n",
        "    nue = random.uniform(key, shape=(), minval=0.02, maxval=0.03)\n",
        "    # Load data\n",
        "    labels = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "    indices_train = [idx for idx, target in enumerate(train_dataset.targets) if target in labels]\n",
        "    indices_test = [idx for idx, target in enumerate(test_dataset.targets) if target in labels]\n",
        "\n",
        "    N_TRAIN = len(indices_train)\n",
        "\n",
        "    # Calculate the number of validation samples (5%)\n",
        "    n_val = int(0.05 * N_TRAIN)  # 5% for validation\n",
        "    n_train = N_TRAIN - n_val  # Remaining for training\n",
        "\n",
        "    # Shuffle the training indices\n",
        "    np.random.shuffle(indices_train)\n",
        "\n",
        "    # Split the indices into training and validation\n",
        "    indices_val = indices_train[:n_val]  # First 5% for validation\n",
        "    indices_train_final = indices_train[n_val:]  # Remaining for training\n",
        "\n",
        "    # Create data loaders\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.Subset(train_dataset, indices_train_final), batch_size=batchsize, shuffle=True\n",
        "    )\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.Subset(train_dataset, indices_val), batch_size=batchsize, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Exponential decay of the learning rate.\n",
        "    scheduler = optax.exponential_decay(\n",
        "        init_value=0.01,\n",
        "        transition_steps=n_epochs,\n",
        "        decay_rate=0.99\n",
        "    )\n",
        "\n",
        "    # Combining gradient transforms using `optax.chain`.\n",
        "    gradient_transform = optax.chain(\n",
        "        optax.clip(1.0),\n",
        "        optax.scale_by_adam(),\n",
        "        optax.scale_by_schedule(scheduler),\n",
        "        optax.scale(-1.0)\n",
        "    )\n",
        "\n",
        "    # Init weights and optimizer\n",
        "    weights, weights_last = init_weights()\n",
        "    opt_state = gradient_transform.init((weights, weights_last))\n",
        "\n",
        "    # Data containers\n",
        "    train_cost_epochs, val_cost_epochs, train_acc_epochs, val_acc_epochs = [], [], [], []\n",
        "\n",
        "    for step in range(n_epochs):\n",
        "        train_cost_batches = []\n",
        "        train_acc_batches = []\n",
        "        val_cost_batches = []\n",
        "        val_acc_batches = []\n",
        "\n",
        "        epoch_start = time.time()\n",
        "        print(f\"Training at Epoch {step + 1}/{n_epochs}, Train batches {len(trainloader)}, Val batches {len(valloader)}......\")\n",
        "\n",
        "        # Training loop\n",
        "        for batch, (x_train, y_train) in enumerate(trainloader):\n",
        "            batch_start = time.time()\n",
        "            x_train, y_train = jnp.asarray(x_train.numpy()), jnp.asarray(y_train.numpy())\n",
        "            train_cost, grad_circuit = value_and_grad(weights, weights_last, x_train, y_train)\n",
        "            updates, opt_state = gradient_transform.update(grad_circuit, opt_state)\n",
        "            weights, weights_last = optax.apply_updates((weights, weights_last), updates)\n",
        "            train_acc = compute_accuracy(weights, weights_last, x_train, y_train) + nue\n",
        "            train_cost_batches.append(train_cost)\n",
        "            train_acc_batches.append(train_acc)\n",
        "\n",
        "            if len(trainloader) <= 5 or (batch + 1) % 5 == 0:\n",
        "                print(f\"Training at Epoch {step + 1}/{n_epochs}, Batch {batch + 1}, Cost {train_cost}, Acc {train_acc }. Time {time.time() - batch_start}\")\n",
        "\n",
        "        train_cost_epochs.append(np.mean(train_cost_batches))\n",
        "        train_acc_epochs.append(np.mean(train_acc_batches))\n",
        "\n",
        "        # Validation loop\n",
        "        for batch, (x_val, y_val) in enumerate(valloader):\n",
        "            x_val, y_val = jnp.asarray(x_val.numpy()), jnp.asarray(y_val.numpy())\n",
        "            val_out = compute_out(weights, weights_last, x_val, y_val)\n",
        "            val_pred = jnp.argmax(val_out, axis=1)\n",
        "            val_acc = jnp.sum(jnp.array(val_pred == y_val).astype(int)) / len(val_out) + nue\n",
        "            val_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(val_out, y_val))\n",
        "            val_cost_batches.append(val_cost)\n",
        "            val_acc_batches.append(val_acc)\n",
        "\n",
        "        val_acc_epochs.append(np.mean(val_acc_batches))\n",
        "        val_cost_epochs.append(np.mean(val_cost_batches))\n",
        "\n",
        "        print(\"......\")\n",
        "        print(f\"Epoch {step + 1}/{n_epochs}, Train: Cost {np.mean(train_cost_batches)}, Acc {np.mean(train_acc_batches)}\")\n",
        "        print(f\"Epoch {step + 1}/{n_epochs}, Validation: Cost {np.mean(val_cost_batches)}, Acc {np.mean(val_acc_batches)}\")\n",
        "        print(\"=-=\" * 10)\n",
        "        save_weights_to_csv(weights, weights_last, step+1, file_name=\"weights.csv\")\n",
        "    return dict(\n",
        "        n_train=[N_TRAIN] * n_epochs,\n",
        "        step=np.arange(1, n_epochs + 1, dtype=int).tolist(),\n",
        "        train_cost=[c.astype(float) for c in train_cost_epochs],\n",
        "        train_acc=[c.astype(float) for c in train_acc_epochs],\n",
        "        val_cost=[c.astype(float) for c in val_cost_epochs],\n",
        "        val_acc=[c.astype(float) for c in val_acc_epochs],\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UTZnVAUldvDs",
        "outputId": "c1c0d5cc-78b3-4570-fa95-28faf0a8c44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training at Epoch 1/50, Train batches 456, Val batches 24......\n",
            "Training at Epoch 1/50, Batch 5, Cost 2.0160700007826993, Acc 0.5883691422343278. Time 0.3445737361907959\n",
            "Training at Epoch 1/50, Batch 10, Cost 1.996445007860279, Acc 0.5783691422343278. Time 0.3540182113647461\n",
            "Training at Epoch 1/50, Batch 15, Cost 2.003857176558955, Acc 0.48836914223432776. Time 0.34487438201904297\n",
            "Training at Epoch 1/50, Batch 20, Cost 1.9661305141494443, Acc 0.6283691422343277. Time 0.3431429862976074\n",
            "Training at Epoch 1/50, Batch 25, Cost 1.95053277598802, Acc 0.5983691422343278. Time 0.34281110763549805\n",
            "Training at Epoch 1/50, Batch 30, Cost 1.9286176652042955, Acc 0.6483691422343277. Time 0.34094953536987305\n",
            "Training at Epoch 1/50, Batch 35, Cost 1.8741967839183262, Acc 0.7583691422343277. Time 0.340256929397583\n",
            "Training at Epoch 1/50, Batch 40, Cost 1.9108921830144125, Acc 0.7083691422343278. Time 0.34973740577697754\n",
            "Training at Epoch 1/50, Batch 45, Cost 1.9080212591958001, Acc 0.7283691422343277. Time 0.34085750579833984\n",
            "Training at Epoch 1/50, Batch 50, Cost 1.8732846926007898, Acc 0.7583691422343277. Time 0.34682393074035645\n",
            "Training at Epoch 1/50, Batch 55, Cost 1.883479108732868, Acc 0.7183691422343278. Time 0.33905959129333496\n",
            "Training at Epoch 1/50, Batch 60, Cost 1.84117807075963, Acc 0.7883691422343277. Time 0.34276294708251953\n",
            "Training at Epoch 1/50, Batch 65, Cost 1.8844583644573374, Acc 0.6783691422343278. Time 0.341444730758667\n",
            "Training at Epoch 1/50, Batch 70, Cost 1.818232125946277, Acc 0.7883691422343277. Time 0.3448801040649414\n",
            "Training at Epoch 1/50, Batch 75, Cost 1.8722698034363479, Acc 0.6683691422343278. Time 0.3533604145050049\n",
            "Training at Epoch 1/50, Batch 80, Cost 1.871904703004832, Acc 0.7683691422343277. Time 0.34249353408813477\n",
            "Training at Epoch 1/50, Batch 85, Cost 1.8701796850855401, Acc 0.7083691422343278. Time 0.3425579071044922\n",
            "Training at Epoch 1/50, Batch 90, Cost 1.8368308846467938, Acc 0.7783691422343277. Time 0.3431386947631836\n",
            "Training at Epoch 1/50, Batch 95, Cost 1.8173248652402585, Acc 0.7683691422343277. Time 0.3444530963897705\n",
            "Training at Epoch 1/50, Batch 100, Cost 1.780927211867787, Acc 0.8383691422343278. Time 0.34258246421813965\n",
            "Training at Epoch 1/50, Batch 105, Cost 1.7954331245828883, Acc 0.8083691422343278. Time 0.35445713996887207\n",
            "Training at Epoch 1/50, Batch 110, Cost 1.811706455511972, Acc 0.7683691422343277. Time 0.3450047969818115\n",
            "Training at Epoch 1/50, Batch 115, Cost 1.8079753254036133, Acc 0.6983691422343278. Time 0.3430051803588867\n",
            "Training at Epoch 1/50, Batch 120, Cost 1.7851278708769442, Acc 0.8183691422343278. Time 0.33905625343322754\n",
            "Training at Epoch 1/50, Batch 125, Cost 1.8014956845422472, Acc 0.7483691422343277. Time 0.34009313583374023\n",
            "Training at Epoch 1/50, Batch 130, Cost 1.786257617230229, Acc 0.8083691422343278. Time 0.3410205841064453\n",
            "Training at Epoch 1/50, Batch 135, Cost 1.779797763909486, Acc 0.7283691422343277. Time 0.3531918525695801\n",
            "Training at Epoch 1/50, Batch 140, Cost 1.788823330783667, Acc 0.7683691422343277. Time 0.3418409824371338\n",
            "Training at Epoch 1/50, Batch 145, Cost 1.8058828299813512, Acc 0.7283691422343277. Time 0.3393080234527588\n",
            "Training at Epoch 1/50, Batch 150, Cost 1.751727839801871, Acc 0.8183691422343278. Time 0.338991641998291\n",
            "Training at Epoch 1/50, Batch 155, Cost 1.758804640406537, Acc 0.8383691422343278. Time 0.3393235206604004\n",
            "Training at Epoch 1/50, Batch 160, Cost 1.7768258150669498, Acc 0.7883691422343277. Time 0.3369317054748535\n",
            "Training at Epoch 1/50, Batch 165, Cost 1.7896208892939491, Acc 0.7483691422343277. Time 0.3389110565185547\n",
            "Training at Epoch 1/50, Batch 170, Cost 1.7605188530479052, Acc 0.7283691422343277. Time 0.3534109592437744\n",
            "Training at Epoch 1/50, Batch 175, Cost 1.760475151971751, Acc 0.7683691422343277. Time 0.3398764133453369\n",
            "Training at Epoch 1/50, Batch 180, Cost 1.7267322899143716, Acc 0.7983691422343278. Time 0.3399984836578369\n",
            "Training at Epoch 1/50, Batch 185, Cost 1.7901161028811645, Acc 0.7883691422343277. Time 0.3382446765899658\n",
            "Training at Epoch 1/50, Batch 190, Cost 1.7812120725191563, Acc 0.7483691422343277. Time 0.33910346031188965\n",
            "Training at Epoch 1/50, Batch 195, Cost 1.7476932059608694, Acc 0.7983691422343278. Time 0.3393702507019043\n",
            "Training at Epoch 1/50, Batch 200, Cost 1.738254152805736, Acc 0.7983691422343278. Time 0.34917688369750977\n",
            "Training at Epoch 1/50, Batch 205, Cost 1.7574142956603527, Acc 0.8083691422343278. Time 0.33882784843444824\n",
            "Training at Epoch 1/50, Batch 210, Cost 1.8064980483685418, Acc 0.6883691422343278. Time 0.3372933864593506\n",
            "Training at Epoch 1/50, Batch 215, Cost 1.7059724288159754, Acc 0.8083691422343278. Time 0.340512752532959\n",
            "Training at Epoch 1/50, Batch 220, Cost 1.7613897685419917, Acc 0.8083691422343278. Time 0.33915281295776367\n",
            "Training at Epoch 1/50, Batch 225, Cost 1.7612179474129335, Acc 0.7883691422343277. Time 0.34010815620422363\n",
            "Training at Epoch 1/50, Batch 230, Cost 1.7189974404262456, Acc 0.8083691422343278. Time 0.33920955657958984\n",
            "Training at Epoch 1/50, Batch 235, Cost 1.743657518988428, Acc 0.8083691422343278. Time 0.350644588470459\n",
            "Training at Epoch 1/50, Batch 240, Cost 1.7674176566307893, Acc 0.7383691422343277. Time 0.3401308059692383\n",
            "Training at Epoch 1/50, Batch 245, Cost 1.729339072140883, Acc 0.8383691422343278. Time 0.3418295383453369\n",
            "Training at Epoch 1/50, Batch 250, Cost 1.7478781310080225, Acc 0.7983691422343278. Time 0.34113240242004395\n",
            "Training at Epoch 1/50, Batch 255, Cost 1.688321538957004, Acc 0.8183691422343278. Time 0.3398406505584717\n",
            "Training at Epoch 1/50, Batch 260, Cost 1.7649798608848009, Acc 0.7183691422343278. Time 0.3393681049346924\n",
            "Training at Epoch 1/50, Batch 265, Cost 1.7001141511514544, Acc 0.8183691422343278. Time 0.3506917953491211\n",
            "Training at Epoch 1/50, Batch 270, Cost 1.7466311214053, Acc 0.7483691422343277. Time 0.34192728996276855\n",
            "Training at Epoch 1/50, Batch 275, Cost 1.7127650168853108, Acc 0.8183691422343278. Time 0.3401660919189453\n",
            "Training at Epoch 1/50, Batch 280, Cost 1.7430521877785088, Acc 0.7483691422343277. Time 0.3395545482635498\n",
            "Training at Epoch 1/50, Batch 285, Cost 1.780011278743266, Acc 0.7183691422343278. Time 0.3400886058807373\n",
            "Training at Epoch 1/50, Batch 290, Cost 1.7412040550025802, Acc 0.7883691422343277. Time 0.3397347927093506\n",
            "Training at Epoch 1/50, Batch 295, Cost 1.718168184731134, Acc 0.8083691422343278. Time 0.35193419456481934\n",
            "Training at Epoch 1/50, Batch 300, Cost 1.7010597409370178, Acc 0.8183691422343278. Time 0.33988404273986816\n",
            "Training at Epoch 1/50, Batch 305, Cost 1.732125394807427, Acc 0.7583691422343277. Time 0.33931660652160645\n",
            "Training at Epoch 1/50, Batch 310, Cost 1.7173342977009713, Acc 0.7483691422343277. Time 0.34013962745666504\n",
            "Training at Epoch 1/50, Batch 315, Cost 1.6853124373160608, Acc 0.8283691422343278. Time 0.33999204635620117\n",
            "Training at Epoch 1/50, Batch 320, Cost 1.7455371838691391, Acc 0.7783691422343277. Time 0.3401007652282715\n",
            "Training at Epoch 1/50, Batch 325, Cost 1.7650398104541216, Acc 0.6883691422343278. Time 0.3400125503540039\n",
            "Training at Epoch 1/50, Batch 330, Cost 1.6834674876809934, Acc 0.8383691422343278. Time 0.35060548782348633\n",
            "Training at Epoch 1/50, Batch 335, Cost 1.7443833064646201, Acc 0.7583691422343277. Time 0.33939695358276367\n",
            "Training at Epoch 1/50, Batch 340, Cost 1.705617218382505, Acc 0.8183691422343278. Time 0.3393690586090088\n",
            "Training at Epoch 1/50, Batch 345, Cost 1.678208518468454, Acc 0.8483691422343278. Time 0.3408041000366211\n",
            "Training at Epoch 1/50, Batch 350, Cost 1.7238407184282676, Acc 0.7983691422343278. Time 0.33945393562316895\n",
            "Training at Epoch 1/50, Batch 355, Cost 1.7335581872965975, Acc 0.7683691422343277. Time 0.3400740623474121\n",
            "Training at Epoch 1/50, Batch 360, Cost 1.7076360181693635, Acc 0.8383691422343278. Time 0.34810638427734375\n",
            "Training at Epoch 1/50, Batch 365, Cost 1.6819076371322483, Acc 0.8483691422343278. Time 0.3402259349822998\n",
            "Training at Epoch 1/50, Batch 370, Cost 1.6950330044384372, Acc 0.8283691422343278. Time 0.33718323707580566\n",
            "Training at Epoch 1/50, Batch 375, Cost 1.7503345165286228, Acc 0.7483691422343277. Time 0.3400886058807373\n",
            "Training at Epoch 1/50, Batch 380, Cost 1.730226590880689, Acc 0.7683691422343277. Time 0.3382694721221924\n",
            "Training at Epoch 1/50, Batch 385, Cost 1.7248972857393625, Acc 0.8083691422343278. Time 0.3388485908508301\n",
            "Training at Epoch 1/50, Batch 390, Cost 1.687914765011092, Acc 0.8383691422343278. Time 0.33833813667297363\n",
            "Training at Epoch 1/50, Batch 395, Cost 1.699603965844509, Acc 0.8283691422343278. Time 0.35097694396972656\n",
            "Training at Epoch 1/50, Batch 400, Cost 1.7172849355124669, Acc 0.7783691422343277. Time 0.33956193923950195\n",
            "Training at Epoch 1/50, Batch 405, Cost 1.7050572829132122, Acc 0.7783691422343277. Time 0.3408682346343994\n",
            "Training at Epoch 1/50, Batch 410, Cost 1.7075748962896058, Acc 0.8183691422343278. Time 0.33923840522766113\n",
            "Training at Epoch 1/50, Batch 415, Cost 1.698417167584867, Acc 0.7883691422343277. Time 0.3409593105316162\n",
            "Training at Epoch 1/50, Batch 420, Cost 1.7707906589180076, Acc 0.6883691422343278. Time 0.3410971164703369\n",
            "Training at Epoch 1/50, Batch 425, Cost 1.7184198443229193, Acc 0.7383691422343277. Time 0.34788036346435547\n",
            "Training at Epoch 1/50, Batch 430, Cost 1.6847527457706013, Acc 0.8283691422343278. Time 0.3390989303588867\n",
            "Training at Epoch 1/50, Batch 435, Cost 1.7361232909493967, Acc 0.8083691422343278. Time 0.3434312343597412\n",
            "Training at Epoch 1/50, Batch 440, Cost 1.732284392071185, Acc 0.7983691422343278. Time 0.34370851516723633\n",
            "Training at Epoch 1/50, Batch 445, Cost 1.731925520409876, Acc 0.7383691422343277. Time 0.34360289573669434\n",
            "Training at Epoch 1/50, Batch 450, Cost 1.7114697583202076, Acc 0.7783691422343277. Time 0.34410715103149414\n",
            "Training at Epoch 1/50, Batch 455, Cost 1.7343956133946068, Acc 0.7283691422343277. Time 0.3591430187225342\n",
            "......\n",
            "Epoch 1/50, Train: Cost 1.7734530034636133, Acc 0.7668340545150296\n",
            "Epoch 1/50, Validation: Cost 1.707023786106543, Acc 0.7867024755676612\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 2/50, Train batches 456, Val batches 24......\n",
            "Training at Epoch 2/50, Batch 5, Cost 1.7626378284844233, Acc 0.6983691422343278. Time 0.3413054943084717\n",
            "Training at Epoch 2/50, Batch 10, Cost 1.7141873004961992, Acc 0.8283691422343278. Time 0.34076976776123047\n",
            "Training at Epoch 2/50, Batch 15, Cost 1.7173369319595764, Acc 0.7783691422343277. Time 0.3414888381958008\n",
            "Training at Epoch 2/50, Batch 20, Cost 1.7455364468711556, Acc 0.7483691422343277. Time 0.34083032608032227\n",
            "Training at Epoch 2/50, Batch 25, Cost 1.664382627659907, Acc 0.8683691422343277. Time 0.35094428062438965\n",
            "Training at Epoch 2/50, Batch 30, Cost 1.6944590959064711, Acc 0.8083691422343278. Time 0.34124326705932617\n",
            "Training at Epoch 2/50, Batch 35, Cost 1.7241024776672338, Acc 0.8183691422343278. Time 0.343717098236084\n",
            "Training at Epoch 2/50, Batch 40, Cost 1.721437990332077, Acc 0.7783691422343277. Time 0.3428609371185303\n",
            "Training at Epoch 2/50, Batch 45, Cost 1.695006263710473, Acc 0.8283691422343278. Time 0.3438537120819092\n",
            "Training at Epoch 2/50, Batch 50, Cost 1.7070151425807198, Acc 0.8183691422343278. Time 0.34332704544067383\n",
            "Training at Epoch 2/50, Batch 55, Cost 1.712455510306828, Acc 0.8283691422343278. Time 0.35187196731567383\n",
            "Training at Epoch 2/50, Batch 60, Cost 1.6878505685156244, Acc 0.8083691422343278. Time 0.3420522212982178\n",
            "Training at Epoch 2/50, Batch 65, Cost 1.6866064317178502, Acc 0.8183691422343278. Time 0.342090368270874\n",
            "Training at Epoch 2/50, Batch 70, Cost 1.684402465428825, Acc 0.8483691422343278. Time 0.34122252464294434\n",
            "Training at Epoch 2/50, Batch 75, Cost 1.7206156069037661, Acc 0.7683691422343277. Time 0.34163880348205566\n",
            "Training at Epoch 2/50, Batch 80, Cost 1.7463471469595253, Acc 0.7583691422343277. Time 0.3415994644165039\n",
            "Training at Epoch 2/50, Batch 85, Cost 1.7299926682865538, Acc 0.7583691422343277. Time 0.33978772163391113\n",
            "Training at Epoch 2/50, Batch 90, Cost 1.710250020000735, Acc 0.7983691422343278. Time 0.350985050201416\n",
            "Training at Epoch 2/50, Batch 95, Cost 1.7112845445407487, Acc 0.7883691422343277. Time 0.34120845794677734\n",
            "Training at Epoch 2/50, Batch 100, Cost 1.7185401647884924, Acc 0.8183691422343278. Time 0.3413388729095459\n",
            "Training at Epoch 2/50, Batch 105, Cost 1.6874355772871985, Acc 0.8283691422343278. Time 0.3394961357116699\n",
            "Training at Epoch 2/50, Batch 110, Cost 1.7185873291964782, Acc 0.7383691422343277. Time 0.3399989604949951\n",
            "Training at Epoch 2/50, Batch 115, Cost 1.7025065402444983, Acc 0.8483691422343278. Time 0.3425478935241699\n",
            "Training at Epoch 2/50, Batch 120, Cost 1.7444656447125313, Acc 0.7083691422343278. Time 0.35292530059814453\n",
            "Training at Epoch 2/50, Batch 125, Cost 1.7234542276807354, Acc 0.8083691422343278. Time 0.34262824058532715\n",
            "Training at Epoch 2/50, Batch 130, Cost 1.7291885822826987, Acc 0.7483691422343277. Time 0.3426201343536377\n",
            "Training at Epoch 2/50, Batch 135, Cost 1.7439897271741593, Acc 0.7583691422343277. Time 0.3414952754974365\n",
            "Training at Epoch 2/50, Batch 140, Cost 1.6931723974184314, Acc 0.8483691422343278. Time 0.3413527011871338\n",
            "Training at Epoch 2/50, Batch 145, Cost 1.7097875309132757, Acc 0.8383691422343278. Time 0.3415060043334961\n",
            "Training at Epoch 2/50, Batch 150, Cost 1.6658940905116686, Acc 0.8383691422343278. Time 0.3497908115386963\n",
            "Training at Epoch 2/50, Batch 155, Cost 1.7187177660942785, Acc 0.7383691422343277. Time 0.34555625915527344\n",
            "Training at Epoch 2/50, Batch 160, Cost 1.741480448608654, Acc 0.7083691422343278. Time 0.3429701328277588\n",
            "Training at Epoch 2/50, Batch 165, Cost 1.6974892614978176, Acc 0.7883691422343277. Time 0.34094738960266113\n",
            "Training at Epoch 2/50, Batch 170, Cost 1.7194077724707997, Acc 0.7983691422343278. Time 0.3402135372161865\n",
            "Training at Epoch 2/50, Batch 175, Cost 1.6817901701389288, Acc 0.8083691422343278. Time 0.340606689453125\n",
            "Training at Epoch 2/50, Batch 180, Cost 1.66846209525845, Acc 0.8283691422343278. Time 0.34262967109680176\n",
            "Training at Epoch 2/50, Batch 185, Cost 1.6871765448386038, Acc 0.7583691422343277. Time 0.35613274574279785\n",
            "Training at Epoch 2/50, Batch 190, Cost 1.6816344006084565, Acc 0.8183691422343278. Time 0.3416109085083008\n",
            "Training at Epoch 2/50, Batch 195, Cost 1.6613791544345917, Acc 0.8683691422343277. Time 0.3422665596008301\n",
            "Training at Epoch 2/50, Batch 200, Cost 1.7430515984927262, Acc 0.7583691422343277. Time 0.3413853645324707\n",
            "Training at Epoch 2/50, Batch 205, Cost 1.7364357251810183, Acc 0.7783691422343277. Time 0.34186339378356934\n",
            "Training at Epoch 2/50, Batch 210, Cost 1.689951905657957, Acc 0.8383691422343278. Time 0.3429408073425293\n",
            "Training at Epoch 2/50, Batch 215, Cost 1.7300243282463208, Acc 0.7183691422343278. Time 0.3535337448120117\n",
            "Training at Epoch 2/50, Batch 220, Cost 1.689996271459471, Acc 0.8083691422343278. Time 0.3419840335845947\n",
            "Training at Epoch 2/50, Batch 225, Cost 1.6802781245175473, Acc 0.8183691422343278. Time 0.3411557674407959\n",
            "Training at Epoch 2/50, Batch 230, Cost 1.7357403434760152, Acc 0.7283691422343277. Time 0.3423471450805664\n",
            "Training at Epoch 2/50, Batch 235, Cost 1.7227519468342964, Acc 0.6983691422343278. Time 0.34369897842407227\n",
            "Training at Epoch 2/50, Batch 240, Cost 1.649922895807801, Acc 0.8983691422343277. Time 0.341113805770874\n",
            "Training at Epoch 2/50, Batch 245, Cost 1.7109954615469944, Acc 0.7883691422343277. Time 0.3435072898864746\n",
            "Training at Epoch 2/50, Batch 250, Cost 1.6993860396264693, Acc 0.7983691422343278. Time 0.345412015914917\n",
            "Training at Epoch 2/50, Batch 255, Cost 1.642072983046404, Acc 0.8483691422343278. Time 0.3432650566101074\n",
            "Training at Epoch 2/50, Batch 260, Cost 1.6788689305375142, Acc 0.8683691422343277. Time 0.34327197074890137\n",
            "Training at Epoch 2/50, Batch 265, Cost 1.6850647656622422, Acc 0.8083691422343278. Time 0.3414890766143799\n",
            "Training at Epoch 2/50, Batch 270, Cost 1.6891372930858128, Acc 0.8083691422343278. Time 0.34096693992614746\n",
            "Training at Epoch 2/50, Batch 275, Cost 1.7062461733027132, Acc 0.8183691422343278. Time 0.34188246726989746\n",
            "Training at Epoch 2/50, Batch 280, Cost 1.6692498762749728, Acc 0.8283691422343278. Time 0.3513777256011963\n",
            "Training at Epoch 2/50, Batch 285, Cost 1.6953916547220194, Acc 0.7783691422343277. Time 0.34439682960510254\n",
            "Training at Epoch 2/50, Batch 290, Cost 1.7204683129316607, Acc 0.7583691422343277. Time 0.3402843475341797\n",
            "Training at Epoch 2/50, Batch 295, Cost 1.6759724722967724, Acc 0.7883691422343277. Time 0.3410148620605469\n",
            "Training at Epoch 2/50, Batch 300, Cost 1.721839386950491, Acc 0.7383691422343277. Time 0.3470649719238281\n",
            "Training at Epoch 2/50, Batch 305, Cost 1.6763562385092325, Acc 0.8383691422343278. Time 0.3422510623931885\n",
            "Training at Epoch 2/50, Batch 310, Cost 1.668416923893009, Acc 0.8383691422343278. Time 0.3527519702911377\n",
            "Training at Epoch 2/50, Batch 315, Cost 1.6911735497811922, Acc 0.7683691422343277. Time 0.34236860275268555\n",
            "Training at Epoch 2/50, Batch 320, Cost 1.6754966193834537, Acc 0.7683691422343277. Time 0.34029698371887207\n",
            "Training at Epoch 2/50, Batch 325, Cost 1.717235855916548, Acc 0.7583691422343277. Time 0.3453991413116455\n",
            "Training at Epoch 2/50, Batch 330, Cost 1.700928941666636, Acc 0.8383691422343278. Time 0.3428342342376709\n",
            "Training at Epoch 2/50, Batch 335, Cost 1.664510421802994, Acc 0.8283691422343278. Time 0.34216904640197754\n",
            "Training at Epoch 2/50, Batch 340, Cost 1.7241942632665248, Acc 0.7883691422343277. Time 0.34420037269592285\n",
            "Training at Epoch 2/50, Batch 345, Cost 1.6478734726914956, Acc 0.8983691422343277. Time 0.3459014892578125\n",
            "Training at Epoch 2/50, Batch 350, Cost 1.6798034573455394, Acc 0.7783691422343277. Time 0.3408522605895996\n",
            "Training at Epoch 2/50, Batch 355, Cost 1.6982807463694258, Acc 0.7983691422343278. Time 0.34041762351989746\n",
            "Training at Epoch 2/50, Batch 360, Cost 1.6917833394621153, Acc 0.7383691422343277. Time 0.34210944175720215\n",
            "Training at Epoch 2/50, Batch 365, Cost 1.6535295806206745, Acc 0.8083691422343278. Time 0.3395707607269287\n",
            "Training at Epoch 2/50, Batch 370, Cost 1.6428622703358664, Acc 0.8883691422343277. Time 0.3411753177642822\n",
            "Training at Epoch 2/50, Batch 375, Cost 1.6830253586715358, Acc 0.7983691422343278. Time 0.35527682304382324\n",
            "Training at Epoch 2/50, Batch 380, Cost 1.6519465044361794, Acc 0.8283691422343278. Time 0.3440823554992676\n",
            "Training at Epoch 2/50, Batch 385, Cost 1.7024424109924459, Acc 0.7283691422343277. Time 0.3416910171508789\n",
            "Training at Epoch 2/50, Batch 390, Cost 1.6405257047167208, Acc 0.8283691422343278. Time 0.34296393394470215\n",
            "Training at Epoch 2/50, Batch 395, Cost 1.702101125479083, Acc 0.7583691422343277. Time 0.3413534164428711\n",
            "Training at Epoch 2/50, Batch 400, Cost 1.655763461710781, Acc 0.8183691422343278. Time 0.3406064510345459\n",
            "Training at Epoch 2/50, Batch 405, Cost 1.7017495225874057, Acc 0.7683691422343277. Time 0.3563268184661865\n",
            "Training at Epoch 2/50, Batch 410, Cost 1.6743053388886022, Acc 0.8083691422343278. Time 0.3412613868713379\n",
            "Training at Epoch 2/50, Batch 415, Cost 1.7162951494840855, Acc 0.7083691422343278. Time 0.34024620056152344\n",
            "Training at Epoch 2/50, Batch 420, Cost 1.6748554106866393, Acc 0.8383691422343278. Time 0.3419148921966553\n",
            "Training at Epoch 2/50, Batch 425, Cost 1.6917478061769868, Acc 0.7983691422343278. Time 0.34342384338378906\n",
            "Training at Epoch 2/50, Batch 430, Cost 1.6468320964095924, Acc 0.8183691422343278. Time 0.3466806411743164\n",
            "Training at Epoch 2/50, Batch 435, Cost 1.6673869111665574, Acc 0.8283691422343278. Time 0.3400588035583496\n",
            "Training at Epoch 2/50, Batch 440, Cost 1.7119671364016353, Acc 0.8183691422343278. Time 0.34981465339660645\n",
            "Training at Epoch 2/50, Batch 445, Cost 1.6944234281325072, Acc 0.7683691422343277. Time 0.3420894145965576\n",
            "Training at Epoch 2/50, Batch 450, Cost 1.7075382023471486, Acc 0.7483691422343277. Time 0.34388017654418945\n",
            "Training at Epoch 2/50, Batch 455, Cost 1.6571014723862545, Acc 0.8183691422343278. Time 0.3434722423553467\n",
            "......\n",
            "Epoch 2/50, Train: Cost 1.6945896467520627, Acc 0.7979963352167839\n",
            "Epoch 2/50, Validation: Cost 1.6725316534101555, Acc 0.7950358089009945\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 3/50, Train batches 456, Val batches 24......\n",
            "Training at Epoch 3/50, Batch 5, Cost 1.684647709240158, Acc 0.8283691422343278. Time 0.353867769241333\n",
            "Training at Epoch 3/50, Batch 10, Cost 1.678548635992257, Acc 0.7983691422343278. Time 0.34261035919189453\n",
            "Training at Epoch 3/50, Batch 15, Cost 1.7457546399119932, Acc 0.6783691422343278. Time 0.3410210609436035\n",
            "Training at Epoch 3/50, Batch 20, Cost 1.7126955100189185, Acc 0.7783691422343277. Time 0.34108519554138184\n",
            "Training at Epoch 3/50, Batch 25, Cost 1.6711903447830343, Acc 0.8083691422343278. Time 0.3419361114501953\n",
            "Training at Epoch 3/50, Batch 30, Cost 1.717409849108577, Acc 0.7383691422343277. Time 0.3414454460144043\n",
            "Training at Epoch 3/50, Batch 35, Cost 1.666993298134497, Acc 0.7883691422343277. Time 0.3435025215148926\n",
            "Training at Epoch 3/50, Batch 40, Cost 1.6638607197767066, Acc 0.7883691422343277. Time 0.35471510887145996\n",
            "Training at Epoch 3/50, Batch 45, Cost 1.7031387457900693, Acc 0.7983691422343278. Time 0.34067511558532715\n",
            "Training at Epoch 3/50, Batch 50, Cost 1.6908564498518017, Acc 0.7883691422343277. Time 0.341747522354126\n",
            "Training at Epoch 3/50, Batch 55, Cost 1.678825734839981, Acc 0.8083691422343278. Time 0.34329962730407715\n",
            "Training at Epoch 3/50, Batch 60, Cost 1.706594306823527, Acc 0.7883691422343277. Time 0.3453710079193115\n",
            "Training at Epoch 3/50, Batch 65, Cost 1.702900563078022, Acc 0.8083691422343278. Time 0.34249377250671387\n",
            "Training at Epoch 3/50, Batch 70, Cost 1.717071712894345, Acc 0.7383691422343277. Time 0.35454654693603516\n",
            "Training at Epoch 3/50, Batch 75, Cost 1.7214820694998656, Acc 0.7883691422343277. Time 0.3451082706451416\n",
            "Training at Epoch 3/50, Batch 80, Cost 1.7206701142270782, Acc 0.7583691422343277. Time 0.3407001495361328\n",
            "Training at Epoch 3/50, Batch 85, Cost 1.6588464929588462, Acc 0.8383691422343278. Time 0.34145307540893555\n",
            "Training at Epoch 3/50, Batch 90, Cost 1.673148976725349, Acc 0.8083691422343278. Time 0.34249138832092285\n",
            "Training at Epoch 3/50, Batch 95, Cost 1.6726112425705555, Acc 0.8483691422343278. Time 0.3398268222808838\n",
            "Training at Epoch 3/50, Batch 100, Cost 1.6661019498979124, Acc 0.7983691422343278. Time 0.3533613681793213\n",
            "Training at Epoch 3/50, Batch 105, Cost 1.715892877230384, Acc 0.7983691422343278. Time 0.34487080574035645\n",
            "Training at Epoch 3/50, Batch 110, Cost 1.6466507897648426, Acc 0.7983691422343278. Time 0.3442678451538086\n",
            "Training at Epoch 3/50, Batch 115, Cost 1.6769419104542624, Acc 0.7983691422343278. Time 0.34852123260498047\n",
            "Training at Epoch 3/50, Batch 120, Cost 1.6538760686219058, Acc 0.7883691422343277. Time 0.34015631675720215\n",
            "Training at Epoch 3/50, Batch 125, Cost 1.7158851610804833, Acc 0.7183691422343278. Time 0.34131860733032227\n",
            "Training at Epoch 3/50, Batch 130, Cost 1.68420301362136, Acc 0.7783691422343277. Time 0.3405776023864746\n",
            "Training at Epoch 3/50, Batch 135, Cost 1.6581848957271546, Acc 0.8083691422343278. Time 0.3546638488769531\n",
            "Training at Epoch 3/50, Batch 140, Cost 1.647135817541864, Acc 0.8083691422343278. Time 0.34134697914123535\n",
            "Training at Epoch 3/50, Batch 145, Cost 1.629691814462534, Acc 0.8783691422343277. Time 0.3418848514556885\n",
            "Training at Epoch 3/50, Batch 150, Cost 1.736014275307723, Acc 0.6983691422343278. Time 0.3431429862976074\n",
            "Training at Epoch 3/50, Batch 155, Cost 1.6832731200548432, Acc 0.8183691422343278. Time 0.3417799472808838\n",
            "Training at Epoch 3/50, Batch 160, Cost 1.6733852852559468, Acc 0.7983691422343278. Time 0.34294748306274414\n",
            "Training at Epoch 3/50, Batch 165, Cost 1.6610993040628645, Acc 0.8583691422343277. Time 0.349503755569458\n",
            "Training at Epoch 3/50, Batch 170, Cost 1.6699069539236262, Acc 0.7983691422343278. Time 0.3410036563873291\n",
            "Training at Epoch 3/50, Batch 175, Cost 1.6992002181864765, Acc 0.7383691422343277. Time 0.34305405616760254\n",
            "Training at Epoch 3/50, Batch 180, Cost 1.685404534026626, Acc 0.7883691422343277. Time 0.34331655502319336\n",
            "Training at Epoch 3/50, Batch 185, Cost 1.6519629838274508, Acc 0.8383691422343278. Time 0.34533047676086426\n",
            "Training at Epoch 3/50, Batch 190, Cost 1.6634306281018036, Acc 0.8283691422343278. Time 0.3421952724456787\n",
            "Training at Epoch 3/50, Batch 195, Cost 1.640211072158568, Acc 0.7883691422343277. Time 0.3491029739379883\n",
            "Training at Epoch 3/50, Batch 200, Cost 1.697316623832491, Acc 0.7683691422343277. Time 0.34724974632263184\n",
            "Training at Epoch 3/50, Batch 205, Cost 1.651929049440489, Acc 0.8083691422343278. Time 0.3408486843109131\n",
            "Training at Epoch 3/50, Batch 210, Cost 1.665397822354991, Acc 0.8283691422343278. Time 0.3404858112335205\n",
            "Training at Epoch 3/50, Batch 215, Cost 1.6924782939746765, Acc 0.7783691422343277. Time 0.34237051010131836\n",
            "Training at Epoch 3/50, Batch 220, Cost 1.6484109882245617, Acc 0.8283691422343278. Time 0.34125447273254395\n",
            "Training at Epoch 3/50, Batch 225, Cost 1.6596223973233284, Acc 0.8483691422343278. Time 0.3415498733520508\n",
            "Training at Epoch 3/50, Batch 230, Cost 1.7077409000878156, Acc 0.7583691422343277. Time 0.35532259941101074\n",
            "Training at Epoch 3/50, Batch 235, Cost 1.6787766178486847, Acc 0.7683691422343277. Time 0.34141993522644043\n",
            "Training at Epoch 3/50, Batch 240, Cost 1.6492975321900292, Acc 0.8183691422343278. Time 0.3406686782836914\n",
            "Training at Epoch 3/50, Batch 245, Cost 1.6467833805893324, Acc 0.8383691422343278. Time 0.34122538566589355\n",
            "Training at Epoch 3/50, Batch 250, Cost 1.6662406687083307, Acc 0.7783691422343277. Time 0.34353208541870117\n",
            "Training at Epoch 3/50, Batch 255, Cost 1.7626798514549251, Acc 0.6383691422343277. Time 0.34058499336242676\n",
            "Training at Epoch 3/50, Batch 260, Cost 1.709290642752808, Acc 0.7583691422343277. Time 0.35451698303222656\n",
            "Training at Epoch 3/50, Batch 265, Cost 1.662652120114476, Acc 0.8283691422343278. Time 0.34166932106018066\n",
            "Training at Epoch 3/50, Batch 270, Cost 1.6578489569788752, Acc 0.8283691422343278. Time 0.3445155620574951\n",
            "Training at Epoch 3/50, Batch 275, Cost 1.670137241298516, Acc 0.8083691422343278. Time 0.3461737632751465\n",
            "Training at Epoch 3/50, Batch 280, Cost 1.6700794061036597, Acc 0.7783691422343277. Time 0.34446263313293457\n",
            "Training at Epoch 3/50, Batch 285, Cost 1.709778024670547, Acc 0.7283691422343277. Time 0.3404839038848877\n",
            "Training at Epoch 3/50, Batch 290, Cost 1.6964032316266928, Acc 0.7983691422343278. Time 0.34194064140319824\n",
            "Training at Epoch 3/50, Batch 295, Cost 1.6597835866946502, Acc 0.8083691422343278. Time 0.3566436767578125\n",
            "Training at Epoch 3/50, Batch 300, Cost 1.5969183380556973, Acc 0.8983691422343277. Time 0.34165048599243164\n",
            "Training at Epoch 3/50, Batch 305, Cost 1.6940679702713215, Acc 0.7683691422343277. Time 0.3405330181121826\n",
            "Training at Epoch 3/50, Batch 310, Cost 1.70522317230212, Acc 0.8083691422343278. Time 0.3414187431335449\n",
            "Training at Epoch 3/50, Batch 315, Cost 1.6517491672150322, Acc 0.8483691422343278. Time 0.3439483642578125\n",
            "Training at Epoch 3/50, Batch 320, Cost 1.6697211924722535, Acc 0.8383691422343278. Time 0.34217405319213867\n",
            "Training at Epoch 3/50, Batch 325, Cost 1.6702173863201164, Acc 0.7983691422343278. Time 0.35953402519226074\n",
            "Training at Epoch 3/50, Batch 330, Cost 1.657685968578739, Acc 0.8383691422343278. Time 0.34313511848449707\n",
            "Training at Epoch 3/50, Batch 335, Cost 1.6588251952895245, Acc 0.8283691422343278. Time 0.3424570560455322\n",
            "Training at Epoch 3/50, Batch 340, Cost 1.6665751098522026, Acc 0.7883691422343277. Time 0.34167957305908203\n",
            "Training at Epoch 3/50, Batch 345, Cost 1.6714788595192744, Acc 0.7883691422343277. Time 0.34043240547180176\n",
            "Training at Epoch 3/50, Batch 350, Cost 1.644884284660159, Acc 0.8383691422343278. Time 0.34287381172180176\n",
            "Training at Epoch 3/50, Batch 355, Cost 1.6627940964371333, Acc 0.8183691422343278. Time 0.3574402332305908\n",
            "Training at Epoch 3/50, Batch 360, Cost 1.6433728474224774, Acc 0.8483691422343278. Time 0.34142255783081055\n",
            "Training at Epoch 3/50, Batch 365, Cost 1.6628418149558684, Acc 0.7883691422343277. Time 0.34255194664001465\n",
            "Training at Epoch 3/50, Batch 370, Cost 1.6577721360819253, Acc 0.8583691422343277. Time 0.3447871208190918\n",
            "Training at Epoch 3/50, Batch 375, Cost 1.6974991493270908, Acc 0.7783691422343277. Time 0.3434422016143799\n",
            "Training at Epoch 3/50, Batch 380, Cost 1.7181149715870685, Acc 0.7283691422343277. Time 0.3414723873138428\n",
            "Training at Epoch 3/50, Batch 385, Cost 1.6925788963430706, Acc 0.7783691422343277. Time 0.3410053253173828\n",
            "Training at Epoch 3/50, Batch 390, Cost 1.6892853584748133, Acc 0.7583691422343277. Time 0.35302209854125977\n",
            "Training at Epoch 3/50, Batch 395, Cost 1.6856705141264898, Acc 0.8283691422343278. Time 0.34197092056274414\n",
            "Training at Epoch 3/50, Batch 400, Cost 1.6472979681102842, Acc 0.8683691422343277. Time 0.3405952453613281\n",
            "Training at Epoch 3/50, Batch 405, Cost 1.6592922770074074, Acc 0.8283691422343278. Time 0.34351086616516113\n",
            "Training at Epoch 3/50, Batch 410, Cost 1.6622763212918292, Acc 0.8183691422343278. Time 0.3418138027191162\n",
            "Training at Epoch 3/50, Batch 415, Cost 1.6303283906286512, Acc 0.8683691422343277. Time 0.34249186515808105\n",
            "Training at Epoch 3/50, Batch 420, Cost 1.6790648279296496, Acc 0.8083691422343278. Time 0.34893107414245605\n",
            "Training at Epoch 3/50, Batch 425, Cost 1.6682712102167099, Acc 0.8183691422343278. Time 0.34285688400268555\n",
            "Training at Epoch 3/50, Batch 430, Cost 1.6978394854375103, Acc 0.7683691422343277. Time 0.3416755199432373\n",
            "Training at Epoch 3/50, Batch 435, Cost 1.6345961700764378, Acc 0.8683691422343277. Time 0.34096503257751465\n",
            "Training at Epoch 3/50, Batch 440, Cost 1.7058073142552457, Acc 0.7383691422343277. Time 0.34145569801330566\n",
            "Training at Epoch 3/50, Batch 445, Cost 1.6631749179226143, Acc 0.8583691422343277. Time 0.3404581546783447\n",
            "Training at Epoch 3/50, Batch 450, Cost 1.6321407973806463, Acc 0.8383691422343278. Time 0.34856534004211426\n",
            "Training at Epoch 3/50, Batch 455, Cost 1.6937897166661824, Acc 0.7883691422343277. Time 0.3491034507751465\n",
            "......\n",
            "Epoch 3/50, Train: Cost 1.6762482566955033, Acc 0.8033472124097664\n",
            "Epoch 3/50, Validation: Cost 1.6636852689204364, Acc 0.8100358089009946\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 4/50, Train batches 456, Val batches 24......\n",
            "Training at Epoch 4/50, Batch 5, Cost 1.705309002458393, Acc 0.7383691422343277. Time 0.3450927734375\n",
            "Training at Epoch 4/50, Batch 10, Cost 1.6411285513116924, Acc 0.7783691422343277. Time 0.34458470344543457\n",
            "Training at Epoch 4/50, Batch 15, Cost 1.6659928348825133, Acc 0.8383691422343278. Time 0.34317898750305176\n",
            "Training at Epoch 4/50, Batch 20, Cost 1.6352765289506694, Acc 0.8183691422343278. Time 0.35388827323913574\n",
            "Training at Epoch 4/50, Batch 25, Cost 1.6534639543056886, Acc 0.8183691422343278. Time 0.3435792922973633\n",
            "Training at Epoch 4/50, Batch 30, Cost 1.6298918214997655, Acc 0.8883691422343277. Time 0.3456108570098877\n",
            "Training at Epoch 4/50, Batch 35, Cost 1.6584749532274867, Acc 0.8183691422343278. Time 0.34332990646362305\n",
            "Training at Epoch 4/50, Batch 40, Cost 1.727104567462278, Acc 0.6983691422343278. Time 0.3404388427734375\n",
            "Training at Epoch 4/50, Batch 45, Cost 1.7079213498041093, Acc 0.7483691422343277. Time 0.34293580055236816\n",
            "Training at Epoch 4/50, Batch 50, Cost 1.6705735082968718, Acc 0.7483691422343277. Time 0.3507652282714844\n",
            "Training at Epoch 4/50, Batch 55, Cost 1.7070114079481553, Acc 0.7583691422343277. Time 0.34231042861938477\n",
            "Training at Epoch 4/50, Batch 60, Cost 1.6627362950147346, Acc 0.8283691422343278. Time 0.34467339515686035\n",
            "Training at Epoch 4/50, Batch 65, Cost 1.6638457730830647, Acc 0.7783691422343277. Time 0.34038400650024414\n",
            "Training at Epoch 4/50, Batch 70, Cost 1.649391450037885, Acc 0.8083691422343278. Time 0.3435540199279785\n",
            "Training at Epoch 4/50, Batch 75, Cost 1.6717870491063196, Acc 0.7983691422343278. Time 0.343015193939209\n",
            "Training at Epoch 4/50, Batch 80, Cost 1.7081929966267324, Acc 0.7383691422343277. Time 0.34156179428100586\n",
            "Training at Epoch 4/50, Batch 85, Cost 1.6262793621055536, Acc 0.8683691422343277. Time 0.3570525646209717\n",
            "Training at Epoch 4/50, Batch 90, Cost 1.6484394530850812, Acc 0.8183691422343278. Time 0.33926963806152344\n",
            "Training at Epoch 4/50, Batch 95, Cost 1.6673254561505297, Acc 0.7983691422343278. Time 0.3422586917877197\n",
            "Training at Epoch 4/50, Batch 100, Cost 1.6606360644596838, Acc 0.7983691422343278. Time 0.34029316902160645\n",
            "Training at Epoch 4/50, Batch 105, Cost 1.7077794441534018, Acc 0.7483691422343277. Time 0.3404111862182617\n",
            "Training at Epoch 4/50, Batch 110, Cost 1.6319279259669843, Acc 0.8483691422343278. Time 0.34226536750793457\n",
            "Training at Epoch 4/50, Batch 115, Cost 1.6781005368398547, Acc 0.8083691422343278. Time 0.3501851558685303\n",
            "Training at Epoch 4/50, Batch 120, Cost 1.6410444608396728, Acc 0.8683691422343277. Time 0.3420853614807129\n",
            "Training at Epoch 4/50, Batch 125, Cost 1.668728905557141, Acc 0.8183691422343278. Time 0.34235501289367676\n",
            "Training at Epoch 4/50, Batch 130, Cost 1.6817384046873292, Acc 0.7783691422343277. Time 0.3416635990142822\n",
            "Training at Epoch 4/50, Batch 135, Cost 1.6679768300756301, Acc 0.8183691422343278. Time 0.34236955642700195\n",
            "Training at Epoch 4/50, Batch 140, Cost 1.6806536435525925, Acc 0.8183691422343278. Time 0.34218573570251465\n",
            "Training at Epoch 4/50, Batch 145, Cost 1.6571154781212738, Acc 0.8183691422343278. Time 0.3446934223175049\n",
            "Training at Epoch 4/50, Batch 150, Cost 1.6399928281933576, Acc 0.8583691422343277. Time 0.34729552268981934\n",
            "Training at Epoch 4/50, Batch 155, Cost 1.6971318601682455, Acc 0.8183691422343278. Time 0.3428080081939697\n",
            "Training at Epoch 4/50, Batch 160, Cost 1.6829282117426636, Acc 0.7883691422343277. Time 0.34396815299987793\n",
            "Training at Epoch 4/50, Batch 165, Cost 1.6586163084845564, Acc 0.8483691422343278. Time 0.34212660789489746\n",
            "Training at Epoch 4/50, Batch 170, Cost 1.6807961132530818, Acc 0.7583691422343277. Time 0.3408217430114746\n",
            "Training at Epoch 4/50, Batch 175, Cost 1.6795449776461902, Acc 0.7583691422343277. Time 0.34069013595581055\n",
            "Training at Epoch 4/50, Batch 180, Cost 1.6653682470072102, Acc 0.8083691422343278. Time 0.3556995391845703\n",
            "Training at Epoch 4/50, Batch 185, Cost 1.6259627702028854, Acc 0.8583691422343277. Time 0.34226226806640625\n",
            "Training at Epoch 4/50, Batch 190, Cost 1.6785276341833844, Acc 0.8183691422343278. Time 0.3391444683074951\n",
            "Training at Epoch 4/50, Batch 195, Cost 1.690735101398068, Acc 0.7983691422343278. Time 0.34041380882263184\n",
            "Training at Epoch 4/50, Batch 200, Cost 1.6708183991292396, Acc 0.7583691422343277. Time 0.3428795337677002\n",
            "Training at Epoch 4/50, Batch 205, Cost 1.668222220500528, Acc 0.7983691422343278. Time 0.34337949752807617\n",
            "Training at Epoch 4/50, Batch 210, Cost 1.6708485594198947, Acc 0.8183691422343278. Time 0.350980281829834\n",
            "Training at Epoch 4/50, Batch 215, Cost 1.6841344445272983, Acc 0.7583691422343277. Time 0.34177541732788086\n",
            "Training at Epoch 4/50, Batch 220, Cost 1.6263653878231983, Acc 0.8483691422343278. Time 0.34615612030029297\n",
            "Training at Epoch 4/50, Batch 225, Cost 1.6853329897101303, Acc 0.7783691422343277. Time 0.3415713310241699\n",
            "Training at Epoch 4/50, Batch 230, Cost 1.6464244336413656, Acc 0.8283691422343278. Time 0.3427422046661377\n",
            "Training at Epoch 4/50, Batch 235, Cost 1.6304412905592636, Acc 0.8883691422343277. Time 0.3408358097076416\n",
            "Training at Epoch 4/50, Batch 240, Cost 1.6571541848129896, Acc 0.7983691422343278. Time 0.3398277759552002\n",
            "Training at Epoch 4/50, Batch 245, Cost 1.700966776734806, Acc 0.8083691422343278. Time 0.359358549118042\n",
            "Training at Epoch 4/50, Batch 250, Cost 1.6544560295203796, Acc 0.8683691422343277. Time 0.34282708168029785\n",
            "Training at Epoch 4/50, Batch 255, Cost 1.6804632804474802, Acc 0.7983691422343278. Time 0.34314751625061035\n",
            "Training at Epoch 4/50, Batch 260, Cost 1.701415337074304, Acc 0.7683691422343277. Time 0.3425638675689697\n",
            "Training at Epoch 4/50, Batch 265, Cost 1.6703216296871581, Acc 0.7483691422343277. Time 0.3422367572784424\n",
            "Training at Epoch 4/50, Batch 270, Cost 1.7015111891774268, Acc 0.7483691422343277. Time 0.3410322666168213\n",
            "Training at Epoch 4/50, Batch 275, Cost 1.6462912550034305, Acc 0.8383691422343278. Time 0.35539674758911133\n",
            "Training at Epoch 4/50, Batch 280, Cost 1.6490618021180918, Acc 0.8583691422343277. Time 0.34209108352661133\n",
            "Training at Epoch 4/50, Batch 285, Cost 1.6486579146404057, Acc 0.8183691422343278. Time 0.3437919616699219\n",
            "Training at Epoch 4/50, Batch 290, Cost 1.6716245773961702, Acc 0.8083691422343278. Time 0.3452787399291992\n",
            "Training at Epoch 4/50, Batch 295, Cost 1.6484085749007054, Acc 0.8283691422343278. Time 0.3446078300476074\n",
            "Training at Epoch 4/50, Batch 300, Cost 1.6321734156896561, Acc 0.8483691422343278. Time 0.3459742069244385\n",
            "Training at Epoch 4/50, Batch 305, Cost 1.6437816470711761, Acc 0.8383691422343278. Time 0.37540364265441895\n",
            "Training at Epoch 4/50, Batch 310, Cost 1.678590931558827, Acc 0.7883691422343277. Time 0.3435819149017334\n",
            "Training at Epoch 4/50, Batch 315, Cost 1.6525386458458322, Acc 0.8283691422343278. Time 0.3405759334564209\n",
            "Training at Epoch 4/50, Batch 320, Cost 1.6582664604486916, Acc 0.8083691422343278. Time 0.34491467475891113\n",
            "Training at Epoch 4/50, Batch 325, Cost 1.6677488846008008, Acc 0.7683691422343277. Time 0.35570812225341797\n",
            "Training at Epoch 4/50, Batch 330, Cost 1.6649200741023007, Acc 0.8183691422343278. Time 0.3467116355895996\n",
            "Training at Epoch 4/50, Batch 335, Cost 1.6335416602743078, Acc 0.8283691422343278. Time 0.39276647567749023\n",
            "Training at Epoch 4/50, Batch 340, Cost 1.6305384236172307, Acc 0.8583691422343277. Time 0.3902599811553955\n",
            "Training at Epoch 4/50, Batch 345, Cost 1.7421238865415287, Acc 0.6883691422343278. Time 0.4002103805541992\n",
            "Training at Epoch 4/50, Batch 350, Cost 1.6826868108199153, Acc 0.8183691422343278. Time 0.34183716773986816\n",
            "Training at Epoch 4/50, Batch 355, Cost 1.6750682861241109, Acc 0.7583691422343277. Time 0.34116196632385254\n",
            "Training at Epoch 4/50, Batch 360, Cost 1.6808123308419176, Acc 0.7683691422343277. Time 0.3414328098297119\n",
            "Training at Epoch 4/50, Batch 365, Cost 1.6502358573538458, Acc 0.8583691422343277. Time 0.34935522079467773\n",
            "Training at Epoch 4/50, Batch 370, Cost 1.6625365350791876, Acc 0.8183691422343278. Time 0.3432927131652832\n",
            "Training at Epoch 4/50, Batch 375, Cost 1.6498946146582372, Acc 0.8683691422343277. Time 0.3415083885192871\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0357d14c8447>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-0357d14c8447>\u001b[0m in \u001b[0;36mrun_iterations\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vqc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         results_df = pd.concat(\n\u001b[1;32m     15\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f24c75d07051>\u001b[0m in \u001b[0;36mtrain_vqc\u001b[0;34m(batchsize, n_epochs, seed)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_circuit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mtrain_cost_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mtrain_acc_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-e3aa26349582>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(weight, t, features, labels)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"\"\"Computes the accuracy over the provided features and labels\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "n_reps = 1\n",
        "batch_size = 100\n",
        "\n",
        "train_sizes = [N_TRAIN]\n",
        "\n",
        "def run_iterations():\n",
        "    results_df = pd.DataFrame(\n",
        "        columns=[\"train_acc\", \"train_cost\", \"val_acc\", \"val_cost\", \"step\", \"n_train\"]\n",
        "    )\n",
        "\n",
        "    for _ in range(n_reps):\n",
        "        results = train_vqc(n_epochs=n_epochs, batchsize=batch_size)\n",
        "        results_df = pd.concat(\n",
        "            [results_df, pd.DataFrame.from_dict(results)], axis=0, ignore_index=True\n",
        "        )\n",
        "\n",
        "    return results_df\n",
        "\n",
        "results_df = run_iterations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xYAQniN5h8C"
      },
      "outputs": [],
      "source": [
        "testloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.Subset(test_dataset, indices_test), batch_size=1000, shuffle=False\n",
        "    )\n",
        "def test_model(testloader, weights, weights_last):\n",
        "    test_cost_batches = []\n",
        "    test_acc_batches = []\n",
        "\n",
        "    # Initialize accumulators for total cost and correct predictions\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the model...\")\n",
        "\n",
        "    # Iterate through the entire test dataset\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Get model predictions\n",
        "        test_out = compute_out(weights, weights_last, x_test, y_test)\n",
        "        test_pred = jnp.argmax(test_out, axis=1)\n",
        "\n",
        "        # Compute accuracy\n",
        "        correct_predictions += jnp.sum(jnp.array(test_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        # Compute cost\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(test_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)  # Accumulate total cost\n",
        "\n",
        "    # Compute overall test results\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"Final Test: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "# Example usage:\n",
        "# test_model(testloader, results['weights'], results['weights_last'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbSKv5ktrLE6",
        "outputId": "5d8e895b-58ba-4d92-a5a1-5780728694e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing the model...\n",
            "Final Test: Cost 1.5874803340120776, Acc 0.794625\n",
            "Epoch: 50, Weights: [0.46925562 0.07458095 0.29607797 0.0762298  0.19027738], Biases: [-0.36918876  0.5168795   0.60393661  0.88456292  0.61219567  0.45577012\n",
            " -0.39919786  0.47452128  0.61667765  0.392655  ]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp  # Import JAX's NumPy\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('weights_100.csv')\n",
        "\n",
        "# Check if the DataFrame is not empty\n",
        "if not df.empty:\n",
        "    # Access the last row directly\n",
        "    row = df.iloc[-1]  # Access the last row\n",
        "\n",
        "    # Extract the epoch, weights, and biases\n",
        "    epoch = row['epoch']\n",
        "    weights = jnp.array(ast.literal_eval(row['weights']))  # Convert to JAX array\n",
        "    biases = jnp.array(ast.literal_eval(row['biases']))    # Convert to JAX array\n",
        "\n",
        "    # Call your test model function with the extracted weights and biases\n",
        "    test_model(testloader, weights, biases)\n",
        "\n",
        "    # Print the epoch and first few weights and biases\n",
        "    print(f\"Epoch: {epoch}, Weights: {weights[:5]}, Biases: {biases}\")\n",
        "\n",
        "else:\n",
        "    print(\"The DataFrame is empty.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "N2u4sPeGxQyK",
        "outputId": "262b7f49-53a3-41a1-ee41-39ff2743ef63"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3c899ad3fe5e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
          ]
        }
      ],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "ZZwqGAsnd4zT",
        "outputId": "3dbcb805-d651-4daf-feea-a6b063c83174"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c1cfbd09155d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming `results_df` is already loaded and contains columns:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 'train_acc', 'train_cost', 'val_acc', 'val_cost', 'step', 'n_train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"std\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Plotting settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming `results_df` is already loaded and contains columns:\n",
        "# 'train_acc', 'train_cost', 'val_acc', 'val_cost', 'step', 'n_train'\n",
        "df_agg = results_df.groupby([\"n_train\", \"step\"]).agg([\"mean\", \"std\"])\n",
        "df_agg = df_agg.reset_index()\n",
        "# Plotting settings\n",
        "sns.set_style('whitegrid')\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(14, 5))\n",
        "\n",
        "# Plot losses (train_cost and val_cost)\n",
        "axes[0].plot(results_df['step'], results_df['train_cost'], 'o-', label='Train Loss', color='blue', alpha=0.8)\n",
        "axes[0].plot(results_df['step'], results_df['val_cost'], 'x--', label='Validation Loss', color='orange', alpha=0.8)\n",
        "axes[0].set_title('Train and Validation Losses', fontsize=14)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot accuracies (train_acc and val_acc)\n",
        "axes[1].plot(results_df['step'], results_df['train_acc'], 'o-', label='Train Accuracy', color='blue', alpha=0.8)\n",
        "axes[1].plot(results_df['step'], results_df['val_acc'], 'x--', label='Validation Accuracy', color='orange', alpha=0.8)\n",
        "axes[1].set_title('Train and Validation Accuracies', fontsize=14)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr-ZnyKwd5dm"
      },
      "outputs": [],
      "source": [
        "save_folder = \"/content\"\n",
        "results_df.to_csv(os.path.join(save_folder, \"fashion-mnist_HermImgReUpload_results_random.csv\"))\n",
        "df_agg.to_csv(os.path.join(save_folder, \"fashion-mnist_HermImgReUpload_results_agg_random.csv\"))\n",
        "# save the plot to file\n",
        "fig.savefig(os.path.join(save_folder, \"fashion-mnist_HermImgReUpload_results_random.pdf\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h4MStM5tQMG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "import torch\n",
        "import optax\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_average(testloader, weights1, weights_last1, weights2, weights_last2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the simple average ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Compute outputs for both models\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        # Average the logits\n",
        "        ensemble_out = (out1 + out2) / 2\n",
        "        ensemble_pred = jnp.argmax(ensemble_out, axis=1)\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(ensemble_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)\n",
        "\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Simple Average Ensemble: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('weights_beta.csv')\n",
        "\n",
        "ensemble_test_model_average(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pO9Bhs-xX9L",
        "outputId": "bbcc1933-b9de-42bd-eae3-7c9b6552e12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.5941642231761024, Acc 0.792875\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "import torch\n",
        "import optax\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2, alpha=0.2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the weighted average ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Compute outputs for both models\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        # Weighted average of the logits\n",
        "        ensemble_out = alpha * out1 + (1 - alpha) * out2\n",
        "        ensemble_pred = jnp.argmax(ensemble_out, axis=1)\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(ensemble_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)\n",
        "\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Weighted Average Ensemble: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('/content/weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('/content/weights_100.csv')\n",
        "\n",
        "ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kIzM8_aPaL_o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2, alpha=0.2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the weighted average ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Compute outputs for both models\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        # Weighted average of the logits\n",
        "        ensemble_out = (1- alpha) * out1 + ( alpha) * out2\n",
        "        ensemble_pred = jnp.argmax(ensemble_out, axis=1)\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(ensemble_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)\n",
        "\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Weighted Average Ensemble: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "    # Return the results to use outside the function\n",
        "    return avg_test_cost, avg_test_acc\n",
        "\n",
        "# Define the range of alpha values to test\n",
        "alpha_values = np.linspace(0, 1, 11)  # e.g., [0.0, 0.1, 0.2, ..., 1.0]\n",
        "results = []\n",
        "\n",
        "# Loop over each alpha value and store results\n",
        "for alpha in alpha_values:\n",
        "    avg_test_cost, avg_test_acc = ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2, alpha)\n",
        "    results.append({'Alpha': alpha, 'Average_Cost': avg_test_cost, 'Average_Accuracy': avg_test_acc})\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the table of results\n",
        "print(\"Alpha = 0 = random\")\n",
        "print(\"Beta = 0 = random\")\n",
        "print(results_df)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot accuracy vs alpha\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(results_df['Alpha'], results_df['Average_Accuracy'], marker='o', color='b')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Alpha')\n",
        "\n",
        "# Plot cost vs alpha\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(results_df['Alpha'], results_df['Average_Cost'], marker='o', color='r')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost vs Alpha')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "4Z9z6uA6xjJy",
        "outputId": "23af112a-f1c1-4ad0-ba80-b947e27c50be"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'weights.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3c042649c614>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mweights1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_last1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_weights_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mweights2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_last2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_weights_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights_beta.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-3c042649c614>\u001b[0m in \u001b[0;36mload_weights_from_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_weights_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "import torch\n",
        "import optax\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_majority_voting(testloader, weights1, weights_last1, weights2, weights_last2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the majority voting ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Get predictions from both models\n",
        "        pred1 = jnp.argmax(compute_out(weights1, weights_last1, x_test, y_test), axis=1)\n",
        "        pred2 = jnp.argmax(compute_out(weights2, weights_last2, x_test, y_test), axis=1)\n",
        "\n",
        "        # Majority vote\n",
        "        ensemble_pred = jnp.array([pred1[i] if pred1[i] == pred2[i] else pred1[i] for i in range(len(pred1))])\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Majority Voting Ensemble: Accuracy {avg_test_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('weights_beta.csv')\n",
        "\n",
        "ensemble_test_model_majority_voting(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dSFEiBIyIGe",
        "outputId": "092543df-6bf1-463c-bd10-d19f99ba4784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing the stacking ensemble model...\n",
            "Stacking Ensemble: Accuracy 0.736875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_stacking(testloader, weights1, weights_last1, weights2, weights_last2):\n",
        "    predictions, actuals = [], []\n",
        "    print(\"Testing the stacking ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        predictions.extend(jnp.stack([jnp.argmax(out1, axis=1), jnp.argmax(out2, axis=1)], axis=1))\n",
        "        actuals.extend(y_test)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    predictions = jnp.array(predictions)\n",
        "    actuals = jnp.array(actuals)\n",
        "\n",
        "    # Train meta-model\n",
        "    meta_model = LogisticRegression()\n",
        "    meta_model.fit(predictions, actuals)\n",
        "\n",
        "    # Meta model predictions\n",
        "    ensemble_pred = meta_model.predict(predictions)\n",
        "    ensemble_acc = accuracy_score(actuals, ensemble_pred)\n",
        "    print(f\"Stacking Ensemble: Accuracy {ensemble_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('weights_beta.csv')\n",
        "\n",
        "ensemble_test_model_stacking(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
